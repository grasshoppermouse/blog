[
  {
    "objectID": "posts/2015-04-19-is-pregnancy-immunosuppression-a-myth/index.html",
    "href": "posts/2015-04-19-is-pregnancy-immunosuppression-a-myth/index.html",
    "title": "Is pregnancy immunosuppression a myth?",
    "section": "",
    "text": "My grad student Caity Placek is working on behavioral immunity during pregnancy. Behavioral immunity refers to psychological adaptations that help defend against pathogens; avoidance of sick people would be a possible example.\nCaity’s results suggest that aversion to meat in pregnancy is related to infection risk, especially in the first trimester. This supports an influential hypothesis put forward in 2000 by Flaxman and Sherman that these aversions are due to the immunosuppression that also occurs early in pregnancy. The rationale is that, during human evolution, meat often harbored pathogens, so immunosuppressed pregnant women should have evolved to avoid it.\nThe idea that pregnant women are immunosuppressed originated with Peter Medawar, who won a Nobel prize for his work on tissue grafts. In 1953, Medawar pointed out that the fetus was like an organ transplant: with half of its genes coming from dad, it produced many proteins that were foreign to mom. Why, then, didn’t mom’s immune system attack the fetus as it would a transplanted organ? In an article billed as “the most influential contribution made to the development of the field of Reproductive Immunology” (Billington 2003), Medawar asked\n\nThe immunological problem of pregnancy may be formulated thus: how does the pregnant mother contrive to nourish within itself, for many weeks or months, a foetus that is an antigenically foreign body?\n\nFor the next 40 years, much research on this “immunological paradox” was conducted as if the fetal ‘allograft’ were like a surgically transplanted organ (Erlebacher 2013). One solution to the paradox offered by Medawar was “the immunological indolence or inertness of the [pregnant] mother.” In other words, to avoid rejecting the fetus as they would a transplanted kidney, pregnant women were naturally immunosuppressed.\nA more recent variant of this hypothesis involved the Th1-Th2 paradigm, in which Th1 cells express proinflammatory cytokines, whereas Th2 cells secrete cytokines promoting humoral immunity, and either pathway can down-regulate the other. The pregnancy state was thought to be a Th2 biased state, to avoid an inflammatory response to the fetus.\nBut are pregnant women immunosuppressed, or in a Th2 biased state? When Caity and I started looking into the literature on pregnancy immunosuppression, we found that opinion seems to have shifted dramatically since 2000. Recent reviews on the topic in the New England Journal of Medicine and the American Journal of Reproductive Immunology are heaping skepticism on the idea that pregnant women are immunosuppressed. For instance, Racicot et al. (2014), writing in the American Journal of Reproductive Immunology, state that\n\nThe old concept that pregnancy is associated with immune suppression has created a myth of pregnancy as a state of immunological weakness and therefore of increased susceptibility to infectious diseases. Today, there is increasing evidence suggesting that this concept is incorrect and the immune system during pregnancy is functional and highly active.\n\nIn the same journal, Silasi et al. (2015) write\n\nOne of the main hypotheses used to explain the increased risk for infection and mortality during pregnancy has been the concept of ‘pregnancy as an immune-suppressed condition’. The ‘paradox of pregnancy’ as a semi-allograft has been approached from the point of perspective of organ transplantation. The view of the fetus as an organ transplant, and the requirement of systemic immune suppression for the success of the transplant, led to the proposal of pregnancy as a condition of systemic immune suppression as a requirement for the success of the pregnancy. From this point of view, similarly as in immune-suppressed patients, pregnancy is in a state of weak immunologic protection.\n\n\nThis concept has been tested for many years in animal models as well as in patients with fertility problems. Unfortunately, after almost 50 years of research following this assumption, there is a lack of evidence to support this hypothesis.\n\nWriting in the New England Journal of Medicine, Kourtis et al. (2014) similarly conclude\n\nElucidation of the immunologic alterations and adaptations that occur during pregnancy suggests that older concepts of pregnancy as a state of systemic immunosuppression are oversimplified. A more useful model may be the view of pregnancy as a modulated immunologic condition, not a state of immunosuppression.\n\nThe evidence against systemic (emphasis on systemic) immunosuppression in pregnancy is pretty compelling.\nThere is little epidemiological evidence that pregnant women are more vulnerable to infection. In a review of pregnancy and infection, Kourtis et al. (2014) state “The fact that pregnant women do not seem, on the basis of epidemiologic evidence, to be more susceptible to infections in general also contradicts [the pregnancy immunosuppression] theory.” Keeping in mind that, for some infectious diseases, there are national and global surveillance programs involving millions of patients, and that public health researchers are particularly concerned about infections during pregnancy, this is a powerful blow to the theory.\nMy first thought was that maybe behavioral immunity compensates for immunosuppression, but no, pregnant women respond well to vaccines: “Adequate immunologic responses to vaccination in pregnant women have been demonstrated in several studies and for several pathogens” (Kourtis et al. 2014), and these responses do not appear to depend significantly on trimester (Pazos et al. 2012).\nFurther, researchers find that the immune system is “functional and highly active” during pregnancy (Racicot et al. 2014). And pregnant women’s immune systems do detect and respond to the fetus. The title of this paper says it all: Fetal-Specific CD8+ Cytotoxic T Cell Responses Develop during Normal Human Pregnancy and Exhibit Broad Functional Capacity (Lissauer et al. 2012).\nWhat changed?\nFirst, it is now recognized that many shifts in immunity during pregnancy are specific to the maternal-fetal interface (the uterus and placenta), and these must be clearly distinguished from systemic immune changes. In fact, the Th1-Th2 model of pregnancy was based mostly on studies of the maternal-fetal interface, and might not apply to systemic immunity (Pazos et al. 2012):\n\nMost evidence supporting a Th2 shifts derives from studies of [the maternal/fetal] interface rather than systemic immunity. Although inflammatory events have been shown to be important at critical times at the beginning and end of gestation, for the most part, the uterine environment is biased toward Th2 [34]. Arguments for a Th2 bias in the periphery are much more contentious [35].\n\nSecond, the fetus and placenta are now seen as an active players that generally cooperate with the maternal immune system to provide immunity during pregnancy (Mor et al. 2010).\nThird, earlier views were heavily influenced by studies of pregnancy complications and loss in humans, as well as mouse models, which might not translate to immune responses during healthy human pregnancies.\nWhat didn’t change?\nHow the mother tolerates the semiallogeneic fetus is still seen as an important and not fully understood problem.\nAlthough pregnant women are generally not more susceptible to infection, there is solid evidence that if they do become infected, the consequences are more severe. Infections with influenza, hepatitis E virus, herpes simplex virus, malaria, measles, smallpox, HIV, varicella, and coccidioidomycosis are all more severe in pregnant women (Kourtis et al. 2014).\nPregnant women do seem to be more susceptible to infection with a few pathogens, in particular malaria and listeriosis (Kourtis et al. 2014). Listeriosis is a food borne bacteria, often found on raw dairy products and meat. Hmmm.\nThere are shifts in immunity during pregnancy. A recent longitudinal study of healthy pregnancies, for example, found that “pregnancy is not a period of immunosuppression but an alteration in immune priorities characterized by a strengthening of innate immune barriers and a concomitant reduction in adaptive/inflammatory immunity in the later stages of pregnancy” (Kraus et al. 2012).\nWhere does this leave Caity? And Flaxman and Sherman? Here are a few obvious possible reinterpretations.\nIn the &gt;100 million years since the evolution of viviparity, sophisticated (but not infallible) mechanisms have evolved in both mother and fetus to allow the immune system to be fully active against pathogens without jeopardizing the fetus. Many of these mechanisms are localized to the uterine environment.\nPregnant women might have evolved an aversion to meat, not because they are more vulnerable to infection, but because the costs of infection are higher during pregnancy. It is also possible that some pregnancy-related aversions are specific to particular pathogens that do more easily infect pregnant women.\nPregnancy-related shifts in immunity imply shifts in trade-offs: improvements to some aspects of immunity but detriments to others. These might be related to the energetic cost of pregnancy, to the challenges of a semiallogeneic fetus, and/or to the need for placental, fetal, and maternal immune mechanisms to coordinate. Perhaps, a la Haig, there are conflicts with paternal genes that pose special challenges.\nFinally, there are still many unknowns and much debate. To give the last word to Medawar, he believed the key factor ensuring the success of gestation was not maternal immunosuppression but rather “the anatomical separation of foetus from mother,” a basic conclusion that, according to Billington (2003), remains substantially valid to this day.\n\nReferences\nBillington, W. D. (2003). The immunological problem of pregnancy: 50 years with the hope of progress. A tribute to Peter Medawar. Journal of Reproductive Immunology, 60(1), 1–11. http://doi.org/10.1016/S0165-0378(03)00083-4\nErlebacher, A. (2013). Mechanisms of T cell tolerance towards the allogeneic fetus. Nature Reviews Immunology, 13(1), 23–33. http://doi.org/10.1038/nri3361\nFlaxman, S. M., & Sherman, P. W. (2000). Morning Sickness: A Mechanism for Protecting Mother and Embryo. The Quarterly Review of Biology, 75(2), 113–148. http://doi.org/10.2307/2664252\nKourtis, A. P., Read, J. S., & Jamieson, D. J. (2014). Pregnancy and Infection. New England Journal of Medicine, 370(23), 2211–2218. http://doi.org/10.1056/NEJMra1213566\nLissauer, D., Piper, K., Goodyear, O., Kilby, M. D., & Moss, P. A. H. (2012). Fetal-Specific CD8+ Cytotoxic T Cell Responses Develop during Normal Human Pregnancy and Exhibit Broad Functional Capacity. The Journal of Immunology, 189(2), 1072–1080. http://doi.org/10.4049/jimmunol.1200544\nMedawar, P. B. (1953, January). Some immunological and endocrinological problems raised by the evolution of viviparity in vertebrates. In Symp Soc Exp Biol (Vol. 7, No. 320, p. 38).\nMor, G., & Cardenas, I. (2010). The Immune System in Pregnancy: A Unique Complexity: IMMUNE SYSTEM IN PREGNANCY. American Journal of Reproductive Immunology, 63(6), 425–433. http://doi.org/10.1111/j.1600-0897.2010.00836.x\nPazos, M., Sperling, R. S., Moran, T. M., & Kraus, T. A. (2012). The influence of pregnancy on systemic immunity. Immunologic Research, 54(1-3), 254–261. http://doi.org/10.1007/s12026-012-8303-9\nPlacek, C. (2015) Fetal protection: The roles of social learning and innate food aversions in South India. The International Society for Evolution, Medicine, & Public Health Inaugural Meeting, March 19-21, 2015 in Tempe, Arizona.\nRacicot, K., Kwon, J.-Y., Aldo, P., Silasi, M., & Mor, G. (2014). Understanding the Complexity of the Immune System during Pregnancy. American Journal of Reproductive Immunology, 72(2), 107–116. http://doi.org/10.1111/aji.12289\nSilasi, M., Cardenas, I., Kwon, J.-Y., Racicot, K., Aldo, P., & Mor, G. (2015). Viral Infections During Pregnancy. American Journal of Reproductive Immunology, 73(3), 199–213. http://doi.org/10.1111/aji.12355"
  },
  {
    "objectID": "posts/2017-10-18-put-your-data-in-an-r-package/index.html",
    "href": "posts/2017-10-18-put-your-data-in-an-r-package/index.html",
    "title": "Put your data in an R package",
    "section": "",
    "text": "Updated: Feb 26, 2025\nI used to write long R scripts that imported data files, created new variables, reshaped the data, reshaped it again, and spit out results along the way, all in one file.\nThat worked so long as I never wanted to use that data again. But what if I did? Should I just tack on more code for the new analysis, and then even more code for yet more analyses? That approach will litter your environment with objects that are irrelevant for, and might even interfere with, a particular analysis. Should I copy all the files into a new directory and then hack away at the code? Now I have two copies of the data – which one is definitive? Should I just treat the original data files as the data? Now I have to repeat the same initial processing steps every time I want to reuse that data."
  },
  {
    "objectID": "posts/2017-10-18-put-your-data-in-an-r-package/index.html#the-solution",
    "href": "posts/2017-10-18-put-your-data-in-an-r-package/index.html#the-solution",
    "title": "Put your data in an R package",
    "section": "The solution",
    "text": "The solution\nFor each new data set I create a new R data package. This package lives in my library along with ggplot2, dplyr, lme4, and all my other packages, and is accessible in any project or analysis with a simple:\n\nlibrary(mydatapackage)\n\nCreating a data package involves some small costs, but these are far outweighed by the benefits."
  },
  {
    "objectID": "posts/2017-10-18-put-your-data-in-an-r-package/index.html#pros",
    "href": "posts/2017-10-18-put-your-data-in-an-r-package/index.html#pros",
    "title": "Put your data in an R package",
    "section": "Pros",
    "text": "Pros\n\nYour data is cleanly separated from your analyses.\nYour data is easily accessible in any future project.\nPackages have a built-in documentation system so you can easily document all your variables. The documentation for each data frame (or other object) is accessible with ?my_df.\nPackages have a versioning system so you can keep track of new versions of your data package.\nShare your data with students and colleagues simply by sharing the package.\nArchive your data in a public repository simply by uploading the package."
  },
  {
    "objectID": "posts/2017-10-18-put-your-data-in-an-r-package/index.html#cons",
    "href": "posts/2017-10-18-put-your-data-in-an-r-package/index.html#cons",
    "title": "Put your data in an R package",
    "section": "Cons",
    "text": "Cons\n\nCreating a package is a few extra steps.\nEvery change to the data package requires a rebuild step before the changes are available in your analyses.\nIf you forget to rebuild, your analyses will be using the outdated version of your data, something that can be hard to detect.\nIn the early phases of an analysis, you will probably be moving code back and forth from your data package to your analysis until you find the sweet spot between processed data and analyzed data."
  },
  {
    "objectID": "posts/2015-05-15-im-a-sucker-for-a-good-theory/index.html",
    "href": "posts/2015-05-15-im-a-sucker-for-a-good-theory/index.html",
    "title": "I’m a sucker for a good theory",
    "section": "",
    "text": "I’m a sucker for a good theory.\nAfter getting my BA, I had no idea what I wanted to do with my life. A friend, perhaps exasperated with my refusal to commit to a career, introduced me to her brother-in-law, Bruce Novak, who had just gotten a position in the UC Berkeley Department of Chemistry. Bruce had recently finished his PhD at Cal Tech (under Grubbs, who, to name drop, would later win the Nobel Prize in Chemistry). Bruce needed someone to set up his lab and get projects started, and he asked me if I wanted to do it. I have no idea why he asked me – I had little more than a high school education in chemistry. Also, Bruce was a polymer chemist. Plastics. There’s a great future in plastics. Just not the future I wanted for myself. But hey, it was a paycheck, so I agreed.\nI soon realized that polymer chemistry was the ultimate geek discipline: you build cool stuff at the molecular level. Some molecules have these properties, others have those properties. If you can hook them together in a polymer, you’ve now got a material that has both.\nBruce had a very clever idea. Polyphenylene sulfide (PPS) is a tough, light brown, sulfur-based organic polymer that is normally an electrical insulator. PPS p-orbitals overlap, however. Oxidation removes electrons from the overlapping orbitals, which then form a molecule-wide electron conduction band, very similar to the conduction band in metals. Upon this so-called “doping,” PPS becomes dark and shiny (just like metal!), and conducts electricity along its backbone. Yes, you can turn some plastics into “metals,” retaining advantages of both.\nHere’s PPS (the bracket with the “n” subscript means repeat n times):\n\n\n\npolyphenylene sulfide\n\n\nThere was another group of sulfur-based molecules with a very similar chemical structure, triarylsulfonium salts, that were photo-reactive:\n\n\n\ntriarylsulfonium_reaction\n\n\nThe key, here, is that when you photolyze this molecule (shine light on it, hv), you knock off one of the rings, creating a radical cation – a charged molecule with an unpaired electron (the S+ with the dot over it). If we could make a polymeric form of this molecule, which we dubbed arylated PPS (APPS), it would be very similar to PPS. When we hit it with light, the unpaired electron on the polymer backbone should convert APPS into a conducting polymer – “photo-doping.” In other words, we could imprint conducting circuits directly into a non-conducting plastic simply by exposing it to light.\nHere was the reaction with which we hoped to create APPS (the polymer on the right hand side):\n\n\n\nreaction\n\n\nPPS was hard to work with. It would only dissolve in chloronapthalene at 220 C, conditions that destroyed the other starting material. After weeks of unsuccessful attempts, my undergraduate assistant, Anand Viswanathan, and I finally made something that looked like a light brown polymer, but unlike PPS, it dissolved in acetone at room temperature, as we predicted APPS would do.\nWhat was it? What is photoreactive? Would it conduct electricity? I quickly made a thin film of the stuff. After it dried, I grabbed the lab multimeter, put it on the ohm setting, and stuck the probes on the film. The display was blank – no measurable conductivity. Good. Then I shined a UV light on the film. It turned dark and shiny. I put the probes on again, and the multimeter display started to blink, settling down in the MΩ range. The film was conducting electricity. Not well–it was almost beyond the sensitivity of the meter–but it was conducting!\nMonths of work followed to confirm we’d made what we thought we’d made, that it reacted with light the way we thought it should, and that electrical conduction was based on that radical on the polymer backbone. I did countless NMR’s, UV/vis spectrograms, elemental analyses, and electron spin resonance analyses that all pointed to the same conclusion: we had made APPS, and when we hit it with UV light, we would cleave off a phenyl group, forming a radical cation on the polymer backbone. The material would become reflective (an indicator that an electron conduction band had formed), and it would conduct electricity. When we quenched the radical cation with water, we eliminated electrical conductivity.\nIt was time to submit, to Science. Bruce gave me the first author slot. He waited patiently for me to finish my draft, and then rewrote the whole thing (precisely one sentence of mine remained).\nWhile our paper was under review at Science, we wanted to determine the true conductivity of APPS, and increase it, if possible. The challenge was that as soon as we photolyzed the film, it would start to reflect light, limiting further photolyzation. We therefore had an extremely thin conducting layer, but our calculation of conductivity was based on the entire thickness of the, mostly unphotolyzed, film; our estimated conductivity value was therefore far too low. This problem was compounded by the fact that our low conductivity was near the limit that my multimeter could detect. Worse, the photolyzed film would immediately start to absorb moisture, which quenched the radical, killing the conductivity. We were confident that if we could solve these problems, we would achieve much higher conductivity than we had seen so far.\nI had tried various quick-and-dirty solutions. I flooded the film with a stream of nitrogen, to keep off moisture, but the building nitrogen was contaminated with water. I pumped my equipment into a dry box, but the static electricity played havoc with my ohm meter. Plus, it was hard to work with a delicate film using large rubber gloves. I built a spinner that created ultra thin films of APPS, but this didn’t seem to improve the conductivity values, probably because my meter wasn’t sensitive enough.\nWe finally decided to do things right. We needed to run our experiments under dry nitrogen from start to finish, with the thinnest possible film, and with a conductivity meter that could accurately measure the very low values we would get with such a thin film.\nWe bought what was basically a $10,000 ohm meter that was orders of magnitude more sensitive than my multimeter, and I built a special airtight chamber with a UV light underneath a quartz plate (which transmits UV light), flooded with nitrogen straight off a liquid nitrogen dewar (dry dry dry). We would spin an ultra thin film of APPS on a quartz slide and let it dry in the chamber under a flood of dry nitrogen. We would then lower the conductivity probes and turn on the UV light. If everything worked, we should measure the true, and hopefully much higher, conductivity.\nThe day I finally had everything assembled, we heard back from Science. Our paper was accepted! The copy editing on our article was even already complete. If we could get the higher conductivity numbers in the paper, that would be icing on the cake.\nI rushed into the lab and got the experiment running. I spun a fresh film and put it in the chamber. After a lengthy purge with dry nitrogen, I lowered the probes and measured the conductivity. Nothing. Good. I then flipped the switch on the UV light, and waited. Still no conductivity.\nHuh?\nI opened the unit to confirm that the probes were in contact with the film, and the conductivity meter started to register. Uh oh. I tried another film. Nitrogen purge + UV: zero conductivity. Open the unit: conductivity. I tried another film. Same thing: no conductivity until I exposed the film to air. And another. Same thing. The implication was obvious, and devastating to the project. I called Bruce in to show him what was happening. By this time, the entire lab had gathered around. Tom, my dour labmate, pronounced the epitaph: “It’s water.”\nSee that MX- in the figure, above right, hovering near the S+? APPS is a salt. Add water to salt, and you get electrical conduction, not by a cool electronic conduction mechanism along the polymer backbone, but by an ordinary ionic mechanism. All along we had been looking at ionic conduction in water absorbed from the air, not electronic conduction.\nWe thought we had ruled out ionic conduction. Ionic conduction would not make our film shiny and reflective. With an ionic conduction mechanism, which would not depend on the presence of a radical, water should have increased conductivity. When we deliberately added water to our film, though, it quenched the radical, and also the conductivity, as expected for the electronic mechanism. Somehow, the small amount of moisture in the air was enough to create a thin salty mixture that would conduct, but adding more water degraded the photolyzed polymer so it no longer did.\nThat afternoon, I flipped back through my lab book, looking at the notes I had taken on hundreds of experiments that mostly yielded positive results. How had I missed this? What I now saw was that every time I got a negative result under exceptionally dry conditions, I had dismissed it: the building nitrogen was wet; the static in the dry box was screwing with my electronics; the conductivity in such thin films was too low for my multimeter. Yet the truth was there, hovering like a Romulan battleship just at the edge of sensor range.\nThe frustrating thing was that all our chemistry was right. We had made what we thought we’d made – a brand new photopolymer – and it reacted with light the way we predicted. Every fact we reported, was, in fact, a fact (and our results were eventually published in a respectable journal). The problem is that the evidence for electronic conduction mechanisms in polymers is almost always circumstantial. The same evidence that demonstrated electronic conduction in other polymers was misleading in our case.\nI had allowed myself to be seduced by a very plausible, and seemingly well-supported, theory.\nDriving home that evening as the sun set over San Francisco Bay, I was upbeat, almost euphoric. The truth was, I didn’t want to be a polymer chemist. Over the past year, I had been sneaking off to attend classes by George Lakeoff, John Searle, and Brent Berlin. I was fascinated by theories of cognition and human nature, and I had been reading voraciously in linguistics, philosophy, and anthropology. If our work had been published in Science I would have felt obligated to continue on the project for at least another year, maybe more. Now, with our submission withdrawn and the project more or less dead, I was free.\nA few weeks earlier, the same friend who had introduced me to Bruce bought me a book she had seen in a Berkeley bookstore: Intimate Fathers by Barry Hewlett, a study of parenting among Aka pygmies. This has “Ed” written all over it, she said. She was right.\nI was jumping, though, from the frying pan into the fire. The cognitive and behavioral sciences, I would soon learn, are, like our APPS Science paper, based on seductive theories backed by circumstantial evidence."
  },
  {
    "objectID": "posts/2015-05-16-pavlovs-dogs-dopamine-and-drug-use/index.html",
    "href": "posts/2015-05-16-pavlovs-dogs-dopamine-and-drug-use/index.html",
    "title": "Pavlov’s dogs, dopamine and drug use",
    "section": "",
    "text": "Findings from studies investigating only stimulants (generally cocaine or amphetamine) were often discussed as though they applied to all addictions, even though there was no evidence for such an assumption. (Nutt et al. 2015)\n\nIs it possible to get all the facts right, but still get the explanation wrong? That can happen to anyone. Nutt et al. (2015), writing in Nature Reviews Neuroscience, argue that it’s happened to an entire subfield, the neuroscience of substance use and addiction.\n\n\n\nOlds and Milner\n\n\nThe neuroscience of substance use is grounded in Pavlovian concepts that wield powerful and persistent influences over the behavioral sciences. The story begins over 60 years ago when James Olds and Peter Milner inserted wires into the septal areas of the brains of four rats. The rats were then tested in a Skinner box with a lever (above, right). When the rats pressed the lever, a small voltage was applied to the wire. The rats responded by pressing the lever hundreds of times an hour. Olds and Milner concluded that the “control exercised over the animal’s behavior by means of this reward is extreme, possibly exceeding that exercised by any other reward previously used in animal experimentation” (Old and Milner 1954, p. 426).\nThe critical neurons turned out to be dopamine neurons in the midbrain. It was also discovered that various drugs of abuse increase dopamine release in this region. These and other findings lead Roy Wise and colleagues to propose the “hedonia” hypothesis: dopamine encoded reward itself; it was a “pleasure” molecule. If so, the role of dopamine in both reinforcement learning and drug use was clear: behaviors that resulted in food or sex – “natural” rewards – cause the release of dopamine in the MDS, whose pleasurable effects reinforce those behaviors. Drugs of abuse – conceptualized as “artificial” rewards that “hijack” the brain – also release dopamine, whose pleasurable effects similarly reinforce drug use.\nThus were born two intimately intertwined theories: the dopamine theory of reinforcement learning, and the dopamine theory of substance use and addiction, each deeply rooted in the stimulus-response paradigm at the core of behaviorism.\nTwenty years of experiments using these and other methods showed that although dopamine plays some important role in reinforcement learning, it does not directly mediate the hedonic effects of rewards. In an interview in Science (Wickelgren, 1997), Roy Wise acknowledged that the hedonia hypothesis was wrong (time to update the NIDA website!).\nOne alternative hypothesis is that mesolimbic dopamine mediates wanting, not liking. Key evidence for this hypothesis is that after 6-hydroxydopamine lesioning of their mesolimbic dopamine neurons, rats still seem to enjoy food and sex, but are no longer motivated to seek them out (Berridge 1996). On this view, drug-induced dopamine release hijacks the brain by inducing craving for the drug, but not pleasure at consuming it (Robinson and Berridge, 1993).\nAnother influential idea, based on electrophysiological recordings of dopamine neurons in monkeys, and with roots in the experiments of Pavlov, is that the phasic activity of mesolimbic dopamine neurons encodes reward prediction error: dopamine neurons spike with unexpected rewards, and their activity is suppressed with unexpected absence of rewards. But they don’t spike when monkeys get an expected reward (for an exhaustive review, see Schultz, in press). According to Glimcher (2011, p. 15647), who eloquently defends this theory,\n\n[I]ntertwining of theory and experiment now suggests very clearly that the phasic activity of the midbrain dopamine neurons provides a global mechanism for synaptic modification. These synaptic modifications, in turn, provide the mechanistic underpinning for a specific class of reinforcement learning mechanisms that now seem to underlie much of human and animal behavior. (emphasis added)\n\nThis ambitious theory has a harder time explaining how drugs hijack the brain (see Schultz, 2011 for some ideas).\nEnter David Nutt and colleagues. Whatever the role of dopamine in reinforcement learning, Nutt et al. argue there are problems with the dopamine theory of drug use that have been swept under the carpet. Foremost among these is that, in experiments in humans that directly measure mesolimbic dopamine concentrations using PET or SPECT scans, some popular drugs of abuse do not increase mesolimbic dopamine much. Here is the key figure in their paper (more negative binding values indicate increased dopamine release):\n\n\n\nNutt et al.\n\n\nAs you can see, whereas amphetamine clearly increases dopamine, morphine and THC mostly do not (and results for nicotine are variable).\nNutt et al. argue that the relationship between stimulants and dopamine is not surprising because these drugs act directly on the dopamine system:\n\n[W]hat was overlooked was the fact that methylphenidate and other stimulants act specifically on the dopamine system to increase dopamine levels. Thus, dopamine must be the proximal mediator of any psychological response to stimulants, and it should not be surprising that the change in striatal dopamine release correlates with the subjective high. (emphasis added)\n\nLower availability of D2/D3 receptors is consistently associated with addiction to some drugs, like cocaine, which supports the dopamine theory of drug use. But this association also presents a paradox:\n\nIf dopamine acting through D2 and/or D3 receptors is necessary to experience a drug high, then lower receptor availability should result in less-rewarding rather than more-rewarding drug effects.\n\nLower D2/D3 availability is also not consistently associated with addiction to other drugs, such as opiates, cannabis, or nicotine.\nNutt et al. conclude:\n\nTellingly, the dopamine theory has not led to any new treatments for addiction. We suggest that the role of dopamine in addiction is more complicated than the role proposed in the dopamine theory of reward. We propose that dopamine has a central role in addiction to stimulant drugs, which act directly via the dopamine system, but that it has a less important role, if any, in mediating addiction to other drugs, particularly opiates and cannabis.\n\nUnlike Nutt et al., I don’t fault drug researchers for the dopamine theory. On the contrary, good theories are ambitious, with clear predictions that can be falsified by evidence. It is also easy, when in the grip of a compelling theory with much supporting evidence, to dismiss findings that contradict the theory.\nStill, by highlighting evidence that contradicts the dominant dopamine model of drug use, Nutt et al. bolster our critique of the dopamine model’s evolutionary rationale that drugs are evolutionary novel, provide no benefits, and “hijack” the brain via these dopamine circuits.\nThe image at the top of this post epitomizes an approach to studying evolved animal mechanisms, such as reinforcement learning mechanisms, that often abstracts away critical details of the environment that selected for the evolved mechanism. Most popular drugs are plant defensive chemicals, or their close chemical analogs (alcohol is the exception). Plant defensive compounds have infused animal diets since at least the time terrestrial plants and animals evolved, about 400 million years ago. My colleagues and I argue that neurobiological theories of drug use, and perhaps also the related theories that ground much animal behavior in a single neurotransmitter, would profit by incorporating, rather than ignoring, the long co-evolution of plants and animals.\n\n\n\nCarboniferious\n\n\n\nReferences\nBerridge, K. C. (1996). Food reward: brain substrates of wanting and liking. Neuroscience & Biobehavioral Reviews, 20(1), 1-25.\nGlimcher, P. W. (2011). Colloquium Paper: Understanding dopamine and reinforcement learning: The dopamine reward prediction error hypothesis. Proceedings of the National Academy of Sciences, 108(Supplement_3), 15647–15654. http://doi.org/10.1073/pnas.1014269108\nHagen, E. H., Roulette, C. J., & Sullivan, R. J. (2013). Explaining Human Recreational Use of “pesticides”: The Neurotoxin Regulation Model of Substance Use vs. the Hijack Model and Implications for Age and Sex Differences in Drug Consumption. Frontiers in Psychiatry, 4. http://doi.org/10.3389/fpsyt.2013.00142\nOlds, J. (1958). Self-Stimulation of the Brain Its Use To Study Local Effects of Hunger, Sex, and Drugs. Science, 127(3294), 315–324. http://doi.org/10.1126/science.127.3294.315\nOlds, J., & Milner, P. (1954). Positive reinforcement produced by electrical stimulation of septal area and other regions of rat brain. Journal of Comparative and Physiological Psychology, 47(6), 419–427.\nNutt, D. J., Lingford-Hughes, A., Erritzoe, D., & Stokes, P. R. (2015). The dopamine theory of addiction: 40 years of highs and lows. Nature Reviews Neuroscience, 16(5), 305-312. http://doi.org/10.1038/nrn3939\nRobinson, T. E., & Berridge, K. C. (1993). The neural basis of drug craving: an incentive-sensitization theory of addiction. Brain Research Reviews, 18(3), 247-291.\nSchultz, W. (2011). Potential Vulnerabilities of Neuronal Reward, Risk, and Decision Mechanisms to Addictive Drugs. Neuron, 69(4), 603–617. http://doi.org/10.1016/j.neuron.2011.02.014\nSchultz, W. (in press). Neuronal reward and decision signals: from theories to data. Physiological Reviews. http://www.pdn.cam.ac.uk/staff/schultz/pdfs%20website/2015%20Schultz%20PhysiolRev%20in%20press.pdf\nWickelgren, I. (1997). Getting the brain’s attention. Science, 278(5335), 35-37.\nFigure 1: Modified from Olds, 1958.\nFigure 2: From Nutt et al., 2015.\nFigure 3: Modified from “Carboniferous Pteridophyta (After Dana)” from a the 1896 edition of Underwood’s Native Ferns and their Allies."
  },
  {
    "objectID": "posts/2019-07-27-about-90-of-the-genome-is-junk-which-is-very-informative-about-ancestry-but-says-little-about-biology/index.html",
    "href": "posts/2019-07-27-about-90-of-the-genome-is-junk-which-is-very-informative-about-ancestry-but-says-little-about-biology/index.html",
    "title": "About 90% of the genome is junk, which is very informative about ancestry but says little about biology",
    "section": "",
    "text": "The genome, taken as a whole, has a profound influence on our biology. Many scientists, myself included, see it as a blueprint for the organism, or perhaps more accurately, as I explain here, a program that generates the entire organism. Why, then, have many geneticists concluded that about 90% of the human genome has no influence on the human organism whatsoever, that it’s “junk”? And if most of the genome is junk, how can it reveal anything about an individual’s ancestry?\nI’ll explain why most genetic differences among individuals and populations are probably differences in junk DNA, and therefore, perhaps counter-intuitively, are incredibly informative about ancestry but say little about biology."
  },
  {
    "objectID": "posts/2019-07-27-about-90-of-the-genome-is-junk-which-is-very-informative-about-ancestry-but-says-little-about-biology/index.html#clue-1-huge-variation-in-eurkaryotic-genome-size",
    "href": "posts/2019-07-27-about-90-of-the-genome-is-junk-which-is-very-informative-about-ancestry-but-says-little-about-biology/index.html#clue-1-huge-variation-in-eurkaryotic-genome-size",
    "title": "About 90% of the genome is junk, which is very informative about ancestry but says little about biology",
    "section": "Clue #1: huge variation in eurkaryotic genome size",
    "text": "Clue #1: huge variation in eurkaryotic genome size\nIn the 1940’s and 50’s, biologists determined that the quantity of DNA was remarkably constant in cells from the same species, and therefore dubbed this quantity the “C-value”, but that it was quite variable across cells from different species. Importantly, the C-value, which we now know is the size of the genome (i.e., the number of nucleotides, or “letters”), bears little obvious relationship to the complexity of the organism. Some amoeba, for instance, which are single-celled organisms, have genomes that are 100x larger than the human genome (Eddy 2012). The human genome contains eight times more DNA than that of a puffer fish but 40 times less than that of a lungfish, and among the &gt;200 salamander genomes analyzed as of 2014, all are between 4 and 35 times larger than the human genome (Palazzo and Gregory 2014). See Figure 1 (note the logarithmic scale).\n\n\n\n\n\nFigure 1: Summary of haploid nuclear DNA contents (“genome sizes”) [in megabases] for various groups of eukaryotes. This graph is based on data for about 10,000 species. There is a wide range in genome sizes even among developmentally similar species, and there is no correspondence between genome size and general organism complexity. Humans, which have an average-sized genome for a mammal, are indicated by a star. Note the logarithmic scale. Figure and caption from Palazzo and Gregory 2014.\n\n\n\n\nWhy would the “blueprints” of some simple organisms be so much larger than those of more complex ones, and why would the genomes of very closely related species often differ so dramatically? It is hard to escape the conclusion that a substantial fraction of a large genome has little, if any, influence on the phenotype of the organism. What, then, explains the expansion in genome size in some lineages?"
  },
  {
    "objectID": "posts/2019-07-27-about-90-of-the-genome-is-junk-which-is-very-informative-about-ancestry-but-says-little-about-biology/index.html#clue-2-pervasive-parasitic-dna",
    "href": "posts/2019-07-27-about-90-of-the-genome-is-junk-which-is-very-informative-about-ancestry-but-says-little-about-biology/index.html#clue-2-pervasive-parasitic-dna",
    "title": "About 90% of the genome is junk, which is very informative about ancestry but says little about biology",
    "section": "Clue #2: pervasive parasitic DNA",
    "text": "Clue #2: pervasive parasitic DNA\nA key property of DNA is that it can copy itself. Typically, this serves to make a new copy of the genome in a daughter cell, but sometimes sections of DNA copy themselves from one part of the genome into another, a type of mutation that often causes disease (Hancks & Kazazian, 2016). These sequences of DNA, termed transposons, are considered to be selfish or parasitic DNA because they have evolved features that increase their own replication at the expense of other genes in the genome, e.g., by inserting themselves into or near a gene and thus disrupting its function, consequently harming the organism itself.\nThere are two major classes of transposons. DNA transposons operate by a cut-and-paste mechanism – so-called “jumping genes” – and make up only about 3% percent of the human genome. Retrotransposons, in contrast, operate by a copy-and-paste mechanism, and therefore increase the size of the genome (see Figure 2). Retrotransposons make up at least 45% of the human genome (Cordaux and Batzer 2009).\n\n\n\n\n\nFigure 2: Retrotransposon. Figure from Wikimedia.org\n\n\n\n\nAlu and LINE-1 retrotransposons in the human genome insert perhaps as frequently as once every 20 births (Cordaux and Batzer 2009). Like other parasites, these copies can themselves make copies, and thus pose an extreme threat to other genes and to the organism itself. Many mechanisms have therefore evolved to neutralize them (Goodier, 2016). To illustrate: due to their activity over the last 150 million years, there are &gt;500,000 LINE-1 elements in the human genome, but less than 100 copies are currently active, i.e., able to replicate within the genome (Cordaux and Batzer 2009).\nThe emerging picture, then, is that over evolution, lineages of retrotransposons have expanded, enlarging the genome (see Figure 3). Inserted elements that disrupted gene function were removed by negative selection (Rishishwar et al., 2017). Others were suppressed by cellular mechanisms, and then began to degrade due to accumulating mutations, becoming what we now call junk DNA (Rishishwar et al., 2017), although there is solid evidence that transposable elements have often been co-opted for the regulation of host genes (Chuong et al. 2017). Some particularly ancient retrotransposons might have degraded so extensively that they are no longer easily recognized. One analysis estimated that perhaps up to 70% of the human genome comprises such repetitive elements (Koning, Gu, Castoe, Batzer, & Pollock, 2011).\n\n\n\n\n\nFigure 3: The expansion of Alu elements in primates. The expansion of Alu subfamilies (Yc1, Ya5a2, Yb9, Yb8, Y, Sg1, Sx and J) is superimposed on a tree of primate evolution. The expansion of the various Alu subfamilies is colour coded to denote the times of peak amplification. The approximate copy numbers of each Alu subfamily are also noted. Mya, million years ago. Figure and caption from Batzer and Deininger 2002."
  },
  {
    "objectID": "posts/2019-07-27-about-90-of-the-genome-is-junk-which-is-very-informative-about-ancestry-but-says-little-about-biology/index.html#clue-3-which-was-misleading-only-1-2-of-dna-codes-for-protein",
    "href": "posts/2019-07-27-about-90-of-the-genome-is-junk-which-is-very-informative-about-ancestry-but-says-little-about-biology/index.html#clue-3-which-was-misleading-only-1-2-of-dna-codes-for-protein",
    "title": "About 90% of the genome is junk, which is very informative about ancestry but says little about biology",
    "section": "Clue #3 (which was misleading): only 1-2% of DNA codes for protein",
    "text": "Clue #3 (which was misleading): only 1-2% of DNA codes for protein\nThe genetic code was cracked in the 1960’s: triplets of nucleotides (A,C,G,T) code for specific amino acids, the building blocks of proteins. Only about 1-2% of the genome codes for proteins, however. It was therefore often reported that the remaining 98-99% of the genome was junk, even though it was known as early as 1961 that some non-coding DNA regulated the expression of protein-coding DNA. By the early 2000’s, however, it was widely recognized that much non-coding DNA plays a critical regulatory role (Gerstein et al., 2007).\nThus, more than 1-2% of the genome is functional, but how much more?"
  },
  {
    "objectID": "posts/2019-07-27-about-90-of-the-genome-is-junk-which-is-very-informative-about-ancestry-but-says-little-about-biology/index.html#clue-4-about-90-of-the-genome-shows-no-evidence-of-sequence-conservation",
    "href": "posts/2019-07-27-about-90-of-the-genome-is-junk-which-is-very-informative-about-ancestry-but-says-little-about-biology/index.html#clue-4-about-90-of-the-genome-shows-no-evidence-of-sequence-conservation",
    "title": "About 90% of the genome is junk, which is very informative about ancestry but says little about biology",
    "section": "Clue #4: about 90% of the genome shows no evidence of sequence conservation",
    "text": "Clue #4: about 90% of the genome shows no evidence of sequence conservation\nThe human genome has about 3 billion nucleotides. Every time the genome is copied, each nucleotide has a small probability that it will be miscopied, i.e., will mutate. In humans, there are a few dozen such mutations with each birth (Jonsson et al., 2017). Mutations that alter organism functions will undergo positive or negative selection. Because most such mutations are harmful — there are many more ways to break something than to improve it — they will typically disappear from the population via negative selection. A fertilized zygote with a mutation in a critical part of the genome might not even successfully develop. DNA sequences in which most mutations face negative selection are termed conserved or constrained.\nMutations that do not alter organism function, on the other hand, e.g., those that that occur in sequences with no influence on the phenotype, will not experience positive or negative selection and will therefore accumulate at a more-or-less constant rate. Junk DNA can therefore be distinguished from functional DNA by its lack of conservation across species.\nProtein coding sequences exhibit a surprising degree of conservation. Yeast and humans diverged about a billion years ago, for example, yet 23% of yeast genes have homologs in humans (i.e., the genes derive from a common ancestor). Of these genes, the amino acid sequences overlap, on average, by about 32%. Even more remarkable, after replacing 414 critical yeast genes with their human homologs, 47% of the human genes functioned and enabled the yeast to survive (Kachroo et al. 2015).\nA disadvantage of methods that assess DNA functionality by comparing degrees of sequence conservation across species is that they cannot identify functional sequences that might be recently acquired in a particular species. There might be functional sequences of DNA in the human genome, for instance, that do not appear in the genomes of chimps or other primates.\nTo assess what fraction of the human genome has been subject to negative selection on both long and short timescales, i.e., is constrained and is therefore likely to be functional, Rand et al. (2014) analyzed DNA sequences across the mammals in several functional classes (i.e., protein coding sequences vs. various types of regulatory sequences that might have been acquired recently). They found that 7.1– 9.2% of the human genome is presently constrained, fairly consistent with previous results that found that between 3% and 15% of the human genome was functional. As expected, protein coding sequences were highly constrained, whereas regulatory sequences experienced more rapid turnover (see Figure 4). This implies, of course, that &gt;90% of human DNA is not functional, i.e., is junk (for similar recent estimates, see Ward and Kellis, 2012; Gulko et al., 2015; and references therein).\n\n\n\n\n\nFigure 4: Model-based inference of turnover by functional class [across the mammals]. Schematic summary of the fraction of constrained sequence that has been retained (saturated colours) or turned over (pastel colours) in the human lineage over time (X-axis, divergence time) and how it has been distributed across various categories of functional element. In addition to showing the reduced quantity of preserved constrained sequence with increasing divergence, we infer the reciprocal quantity of sequence that is assumed to have been gained over human lineage evolution. For consistency this approach requires mutually exclusive annotation sets, in contrast to those used in Figure 3, making the results not directly comparable. Overlaps between the major different annotations are shown in Figure S10. Figure and caption from Rands et al. 2014."
  },
  {
    "objectID": "posts/2018-10-09-suicide-and-metoo/index.html",
    "href": "posts/2018-10-09-suicide-and-metoo/index.html",
    "title": "Suicide and #MeToo",
    "section": "",
    "text": "What single recent event in a woman’s life is most strongly associated with her subsequent suicide attempt? A sexual assault.\nIn a large, nationally representive study in France, for instance, the odds that a woman would attempt suicide were 17 times higher if she had experienced a sexual assault in the last year than if she had not, the most potent risk factor among those examined. In fact, although sexual assault is associated with many poor mental health outcomes, such as depression, anxiety, and PTSD, it is most powerfully associated with suicidality.\nSexual assault often causes suicidal behavior, my graduate student Kristen Syme and I argue, precisely because women who report sexual assault are frequently not believed. An assault victim’s suicidal behavior is a last-ditch effort to convince others that she is telling the truth.\nOur hypothesis is based on costly signaling theory: When individuals have conflicts of interest and incentives to lie, costly signals can still be trusted.\nTo illustrate: Imagine that Molly is sexually assaulted by her mother’s boyfriend, Mike, but there are no witnesses. Molly tells her mother about the assault, but her mother doesn’t believe her. Molly blamed Mike for her parents’ divorce. The mother suspects Molly is falsely accusing Mike of sexual assault to force her to break up with him and spend more time with Molly.\nMolly, on the other hand, knows that Mike will probably attack her again. She is already traumatized, and if she gets pregnant by Mike, her life will be ruined. If she can convince her mother that she is telling the truth, she hopes her mother will put her daughter’s welfare first, and break up with Mike.\nMolly swallows a bottle of pills shortly before her mother gets home from work. Molly’s suicide attempt, we argue, is a costly signal to her mother that she is telling the truth.\nThe logic is as follows. For Molly, the cost of a suicide attempt is low: if her mother doesn’t get rid of Mike, Molly’s life is probably ruined anyway. Her future is dim, and she is indifferent between being raped again and dying.\nBut if Molly were lying, if she had not been attacked and did not face any risk of being attacked, then the cost of a suicide attempt would be high: Molly is a young healthy women who, yes, isn’t getting as much attention from her mother as she would like, but her future is bright. A suicide attempt is too costly for her.\nThe fact that Molly would only attempt suicide if she faced a real risk of future attack by Mike convinces her mother that she is telling the truth. Her mother breaks up with Mike.\nOur hypothesis requires that attempts are much more common than completions. The costly signal only succeeds if the victim survives her attempt and her social partners consequently make changes that benefit her. Attempts are indeed vastly more common than completions, especially among young women:\n\n\n\n\n\nFigure 1: Population rates of suicide attempts and completions in the US, 2001-2011. Note the especially high rate of attempts during the years of highest reproductive value: late adolescence to the mid forties. Data from CDC (2014). Non-fatal self-harm based on data from hospital emergency departments on confirmed or suspected injury or poisoning resulting from a deliberate violent act inflicted on oneself with the intent to take one’s own life or with the intent to harm oneself. Mortality data come from the National Center for Heath Statistics. Figure and caption from Syme et al. 2016.\n\n\n\n\nOur theory is far from proved. You can read more about it and supporting evidence in this paper and in an earlier paper on self harm. But, despite over half a century of research, the mainstream view that suicidality is a type of psychopathology, that victims’ brains are dysfunctioning and must be fixed with drugs or therapy, has also not come close to being proved.\nIf we are correct, there is nothing wrong with the victim. Instead, there is a profound problem in her social environment. The most effective response to her suicidal behavior would be to substantially improve her life.\nIn particular, victims of physical or sexual assault — perhaps the strongest risk factors for suicidal behavior — need those closest to them to believe them and protect them. Therapy can be enormously helpful, not because it fixes a broken brain, but because it provides exactly the understanding and support that victims desperately need.\nIt is time to ask: does the mainstream model of suicide, by focusing on the putative psychopathology of the victim, help silence her cry for help?"
  },
  {
    "objectID": "posts/2019-06-20-a-theory-of-natural-selection-5th-century-bc/index.html",
    "href": "posts/2019-06-20-a-theory-of-natural-selection-5th-century-bc/index.html",
    "title": "A theory of natural selection, 5th century BC",
    "section": "",
    "text": "How came the bodies of animals to be contrived with so much art, and for what ends were their several parts?\nIsaac Newton, Opera Omnia, IV. 237.\n\nI’ve always been curious how far back in history the tool-like or machine-like properties of living things were recognized, and how these were explained.\nIt turns out that Darwin was scooped over 2000 years ago.\nPlato clearly recognized that animals have functional traits that are usually, but not always, explained by the benefit they provide to the animal itself. Plato’s Protagoras, written c. 390 BC, relates a version of the Prometheus creation myth in which various properties of animals are explained by their role in animal survival, e.g., an animal’s strength, speed, size, or weaponry is there to protect the animal. Thick hair or hard skins are there to protect against the seasons sent by Zeus. The animals are appointed different foods: some grass, others fruits. Those who eat other animals are few in number, whereas prey are bestowed with fertility so as to preserve the species.\nPlato, however, did not limit beneficial “design” to living things, nor did animal traits always benefit the animal itself. In the Timaeus, written many years later, Plato proposed that a divine and supremely good Demiurge (craftsman) created the universe in precisely such a way that the universe as a whole, as well as its various parts, produce a vast number of good effects. He created the sun, moon, and stars to mark time, for instance, which itself came into being as an image of eternity (Zeyl and Sattler 2017). He created eyes for sight, “to the end that we might behold the courses of intelligence in the heaven, and apply them to the courses of our own intelligence which are akin to them,” that we “might imitate the absolutely unerring courses of God and regulate our own vagaries” (emphasis added).\nAncient atomists such as Democritus, a contemporary of Plato, eschewed such teleology. Instead, according to them, the universe is explained by the movements and interactions of a small number of indivisible particles in a void. The atomists offered ingenious arguments to explain many natural phenomena in terms of interactions of these indivisible particles – atoms – which have only a few intrinsic properties like size and shape, and strike against one another, rebounding and interlocking in an infinite void. Critically, there is no Demiurge. Causation is due only to blind necessity or chance (Berryman 2016).\nAristotle, perhaps writing around 330 BC, grappled with both the atomists and with Plato. To explain some phenomena, even those that benefited humanity, he invoked atomist arguments. Rain, for example, occurs of necessity, and itself has necessary and chance effects (Physics II 8):\n\n[W]hy should not nature work, not for the sake of something, nor because it is better so, but just as the sky rains, not in order to make the corn grow, but of necessity? What is drawn up must cool, and what has been cooled must become water and descend, the result of this being that the corn grows. Similarly if a man’s crop is spoiled on the threshing-floor, the rain did not fall for the sake of this – in order that the crop might be spoiled – but that result just followed.\n\nParts of animals, though, according to Aristotle, cannot be explained (only) by blind necessity or chance. Teeth, for instance:\n\n…are admirably constructed for their general office, the front ones being sharp, so as to cut the food into bits, and the hinder ones broad and flat, so as to grind it to a pulp…. (Parts of Animals III 1).\n\nBecause teeth grow this way in (almost) all humans, Aristotle goes on to argue, this pattern cannot be due to chance, contrary to the atomists. Instead, using a series of abductive arguments, Aristotle concludes that the parts of animals can only be explained by their purpose, a final cause (Ariew 2002). But no inference is made to a Platonic demiurge. Aristotle’s teleology, unlike Plato’s, is local, not global; it is immanent in the organism, not external to it (Ariew 2002; Schiefsky 2007).\nHow, though, did the atomists, who aimed to explain everything by blind necessity or chance, and certainly did not invoke final causes, explain the parts of animals? In a brief but remarkable passage, Aristotle refers to an argument by Empedocles, who lived in the 5th century BC:\n\nWherever then all the parts [of animals] came about just what they would have been if they had come be for an end, such things survived, being organized spontaneously [i.e., by chance] in a fitting way; whereas those which grew otherwise perished and continue to perish, as Empedocles says his ‘man-faced ox-progeny’ did. (Physics II 8)\n\nWhen I read this, my heart skipped a beat. That is natural selection.\nCould I have just made the remarkable discovery that Darwin was scooped by Empedocles in the 5th century BC? Uh, no. A quick google revealed that Darwin himself cited this passage in the preface to 4th edition of Origin as a historical forerunner to his theory (for details, see Gotthelf 2012). Even the Wikipedia article on natural selection mentions it. Dang!\nIt was Empedocles who claimed that everything is composed of exactly four elements – fire, air, earth, and water – which are moved by two opposing forces, Love and Strife. The four elements combine under the force of Love, and separate under the force of Strife (Perry 2016). Empedocles set forth his philosophy in poetry, only fragments of which survive. The lines describing the origins of animal parts are pretty trippy (Perry 2016):\n\nHere sprang up many faces without necks, arms wandered without shoulders, unattached, and eyes strayed alone, in need of foreheads (B 57).\n\nUnder the force of Love, these parts randomly combine:\n\nMany creatures were born with faces and breasts on both sides, man-faced ox-progeny, while others again sprang forth as ox-headed offspring of man, creatures compounded partly of male, partly of the nature of female, and fitted with shadowy parts. (B 61)\n\nIt is these random creatures that differentially survive if, by chance, they were organized in a “fitting way.”\nThis left me wondering: in a materialist account of the universe, which would only arise again in the West in full force following Galileo and Newton, is the idea of natural selection, in some sense, inevitable?\n\nReferences\nAriew, A. (2002). Platonic and Aristotelian roots of teleological arguments. Functions: New readings in the philosophy of psychology and biology, 7-32.\nBerryman, Sylvia, “Ancient Atomism”, The Stanford Encyclopedia of Philosophy (Winter 2016 Edition), Edward N. Zalta (ed.). https://plato.stanford.edu/archives/win2016/entries/atomism-ancient/\nGotthelf, A. (2012). Teleology, First Principles, and Scientific Method in Aristotle’s Biology. Oxford University Press. https://doi.org/10.1093/acprof:oso/9780199287956.001.0001\nParry, Richard, “Empedocles”, The Stanford Encyclopedia of Philosophy (Fall 2016 Edition), Edward N. Zalta (ed.), URL = https://plato.stanford.edu/archives/fall2016/entries/empedocles/.\nSchiefsky, M. (2007). Galen’s teleology and functional explanation. Oxford Studies in Ancient Philosophy, 33, 369-400.\nZeyl, Donald and Sattler, Barbara, “Plato’s Timaeus”, The Stanford Encyclopedia of Philosophy (Winter 2017 Edition), Edward N. Zalta (ed.). https://plato.stanford.edu/archives/win2017/entries/plato-timaeus/."
  },
  {
    "objectID": "posts/2019-07-12-should-scientific-publishing-move-to-github-and-friends/index.html",
    "href": "posts/2019-07-12-should-scientific-publishing-move-to-github-and-friends/index.html",
    "title": "Should scientific publishing move to Github and friends?",
    "section": "",
    "text": "TL;DR: Open access publishing has very high administrative overhead and is therefore too expensive. Github and similar services have substantial and perhaps insurmountable technical and funding advantages as publishing platforms. Scientific publications should therefore be git repos, created by the researchers themselves, that contain the manuscript, data, and analysis code, and that are hosted on, e.g., github, gitlab, bitbucket, sourceforge, and a few others. A ‘journal’ would just be a managed collection of repos. Reviews would be handled via issues. Long-term archiving and minting of doi’s would be handled by zenodo or equivalent data archiving services. Journal reputations would be based on the reputations of the editors, who are typically researchers themselves.\nJuly 17, 2019: Added links at the end to researchers and journals that are already publishing on Github.\nElsevier, the world’s largest publisher of scientific articles, just cut off access to its journals by the University of California, one of the world’s largest producers of scientific articles, because UC objected to Elsevier’s increasingly exorbitant subscription fees (update: agreement reached March 16, 2021). Meanwhile, many funders of scientific research, mostly in Europe but also including the Gates Foundation, have signed on to Plan S, which stipulates that research that is funded with money from supporting institutions must be published in open access journals or platforms.\nOpen access journals are expensive, though, typically charging $1000 or more per article. The reason they are so expensive is that administration and software development is expensive. arXiv.org, the famous physics/math preprint server that hosts articles for free, has a relatively small leadership team of six people, yet salaries alone amount to more than $1.3 million/year. Add in indirect costs, and the total is about $2 million/year, covered by grants, memberships, and Cornell. Their servers and misc expenses are less than 1/10 of the total.\nPLOS, PeerJ, and other open access journals cover their substantial administrative and development costs with publication fees that range from $1000-$3000/article, which is about what traditional journal publishers like Elsevier charge for open access.\nThe Center for Open Science (COS) and osf.io offer free preprint, preregistration, and file hosting services (full disclosure: I use osf.io, and we received one of their $1000 preregistration awards). They currently have about 50 employees and are spending in the neighborhood of $7 million/year, which, as far as I can tell, comes mostly from grants. As COS itself admits, sustainability is a major concern, and will probably involve charging fees to stakeholder communities, e.g., universities.\nIn sum, the multiple open science initiatives each have their own admin teams, incurring high administrative overhead, and are chasing a relatively small pool of users, many of whom have little funding or incentive to contribute to these public goods."
  },
  {
    "objectID": "posts/2019-07-12-should-scientific-publishing-move-to-github-and-friends/index.html#publishing-with-jupyter-notebooks",
    "href": "posts/2019-07-12-should-scientific-publishing-move-to-github-and-friends/index.html#publishing-with-jupyter-notebooks",
    "title": "Should scientific publishing move to Github and friends?",
    "section": "Publishing with Jupyter notebooks:",
    "text": "Publishing with Jupyter notebooks:\n\nThe Scientific Paper Is Obsolete (Atlantic)\nBy Jupyter–Is This the Future of Open Science?\nReproducible academic publications"
  },
  {
    "objectID": "posts/2018-08-23-most-shooters-are-suicidal-would-arming-teachers-deter-them/index.html",
    "href": "posts/2018-08-23-most-shooters-are-suicidal-would-arming-teachers-deter-them/index.html",
    "title": "Most shooters are suicidal. Would arming teachers deter them?",
    "section": "",
    "text": "The FBI recently released a report on pre-attack behaviors of active shooters. The report analyzes the 63 shooters between 2000 and 2013 who had case files, which contain interviews with friends, family members and other background information. The case files were used to identify behaviors and characteristics that might help identify potential shooters before they strike.\nThe statistic that jumped out at my grad student, Kristen Syme, and me was this: many, perhaps most, shooters were suicidal. Of the 35 shooters for whom a determination could be made, 30 had suicidal ideation or engaged in suicide-related behavior prior to the attack (there was no information for the other 28 shooters). Shooters’ circumstances matched those of other suicidal young people, including many cases in the ethnographic record: they were in conflict with powerful others (Syme, Garfield and Hagen, 2016). The FBI report found that most shooters were not motivated by ideology or hatred of a group but instead had strong personal grievances, such as being subjected to disciplinary actions at school or interpersonal conflicts.\nAlthough President Trump and others have argued that arming teachers would deter shooters, it’s hard to deter an individual who seeks death."
  },
  {
    "objectID": "posts/2018-08-23-most-shooters-are-suicidal-would-arming-teachers-deter-them/index.html#could-arming-teachers-encourage-suicidal-shooters",
    "href": "posts/2018-08-23-most-shooters-are-suicidal-would-arming-teachers-deter-them/index.html#could-arming-teachers-encourage-suicidal-shooters",
    "title": "Most shooters are suicidal. Would arming teachers deter them?",
    "section": "Could arming teachers encourage suicidal shooters?",
    "text": "Could arming teachers encourage suicidal shooters?\nA second key statistic from a different set of studies is that many police shootings are “suicide by cop.” That is, a suicidal person who can’t take his or her own life deliberately provokes a police officer to use lethal force against them, usually by threatening the officer with a loaded gun or other weapon. The typical victim of suicide-by-cop is a young adult white male with romantic relationship conflicts. Estimates of the percentage of police killings that are suicide-by-cop vary widely, from 10% to almost 50%, with several estimates in the 30-40% range (Patton and Fremouw).\nArming teachers means millions of teenagers who would only rarely interact with armed police would now regularly interact with armed teachers with whom they might be in conflict. Suicidality is not rare among teenagers. Of the 7.5 million boys in high school, about 10% have seriously considered suicide or made a plan, and 5% have attempted suicide (Lowry et al. 2014). Thus, arming teachers would dramatically increase the opportunities to commit suicide-by-cop, except in this case it would be suicide-by-teacher. If even a tiny fraction of suicidal male teenagers provoked shootouts with teachers and staff at their schools, or simply tried to obtain their guns, it would substantially increase the number of school shootings and deaths."
  },
  {
    "objectID": "posts/2018-08-23-most-shooters-are-suicidal-would-arming-teachers-deter-them/index.html#would-arming-teachers-begin-to-create-an-informal-institution-of-the-active-shooting",
    "href": "posts/2018-08-23-most-shooters-are-suicidal-would-arming-teachers-deter-them/index.html#would-arming-teachers-begin-to-create-an-informal-institution-of-the-active-shooting",
    "title": "Most shooters are suicidal. Would arming teachers deter them?",
    "section": "Would arming teachers begin to create an informal institution of the active shooting?",
    "text": "Would arming teachers begin to create an informal institution of the active shooting?\nA more important consideration, perhaps, is the implication of arming teachers for our institutions of conflict resolution. The FBI report and Kristen’s work on suicide (Syme, Garfield and Hagen, 2016) both indicate that shooters in particular, and suicidal individuals more generally, are engaged in interpersonal conflict.\nAnthropologists have documented numerous examples of cultural institutions that aim to resolve personal conflicts, and prevent them from devolving into blood feuds that engulf entire communities. These institutions often involve personal combat.\nThe Yanomamo, for example, have a graded series of mechanisms to resolve conflict between two parties, ranging from side-slapping to club fights to ax fights. Phillip Walker documented healed depressed cranial fractures in prehistoric skeletal remains from the Channel Islands off the coast of southern California that could be evidence of club fighting similar to that observed among the Yanomamo.\n\n\n\nScars from club fighting in a Yanomamo man.\n\n\nIn Europe and the United States, dueling with swords or pistols historically served to resolve personal conflicts. Though frequently outlawed, dueling persisted for centuries in Europe and was popular in the US, especially the South, up until the Civil War. The toll was substantial. According to Drake (2004), “between 1798 and the Civil War, the [US] Navy lost two-thirds as many officers to dueling as it did to more than 60 years of combat at sea. Many of those killed and maimed were teenage midshipmen and barely older junior officers, casualties of their own reckless judgment….”\nEven today, personal combat to resolve conflicts and restore personal honor is common in US teenagers. Most American males have witnessed, and probably participated in, clandestine fist fights arranged to take place after school. The key point is that aggrieved individuals will attempt to resolve their conflicts using the institutions provided by their culture. An angry Yanomamo would not think to challenge an opponent to rapiers at dawn, nor would a Renaissance nobleman have thought to challenge his opponent to a club fight. Instead, they, and we, choose from the options our cultures provide us.\nSemi-automatic weapons expand the scope of such conflict resolution strategies, enabling aggrieved individuals to effectively attack entire ingroups (school shootings?) or, perhaps as a way to increase one’s ingroup status, kill many members of an outgroup.\nMy concern is that arming teachers would not only fail to deter suicidal shooters and dramatically increase the exposure of suicidal teenagers to folks with loaded firearms, it would also be fateful step towards creating and advertising a new informal institution of conflict resolution that is similar to, but much more deadly than a duel: the active shooting.\nEdited Aug 24, 2018 to add high school suicide stats, Aug 25, 2018 to add subheadings and a bit more detail on dueling, and Aug 4, 2019 to add note on active shooting to resolve conflicts between individuals and groups. Edited Sep 9, 2021 to tweak title."
  },
  {
    "objectID": "posts/2015-06-27-monkey-butts-menstrual-cycles-sex-and-the-color-pink-the-statistical-crisis-in-science/index.html",
    "href": "posts/2015-06-27-monkey-butts-menstrual-cycles-sex-and-the-color-pink-the-statistical-crisis-in-science/index.html",
    "title": "Monkey butts, menstrual cycles, sex, and the color pink. The statistical crisis in science",
    "section": "",
    "text": "A couple of years ago, when I was starting a new project that used cross-national data, I picked up a popular statistics textbook by Andrew Gelman and Jennifer Hill because Gelman is known for his analysis of voting patterns across US states, which is conceptually similar to analysis of data across countries. I decided to google Gelman, a prominent statistics professor at Columbia, to see if he had papers on his website that would provide detailed, published examples of these kinds of analyses (’bout ready to head back to Facebook?).\nIt turned out Gelman is a prolific blogger. At the time, in Slate and on his blog, Gelman was accusing authors of four papers of shoddy stats. One paper was the infamous paper on ESP by Daryl Bem that had already been debunked. But the three other papers were on evolutionary psychology, and two authors of one paper were my former advisors John Tooby and Leda Cosmides.\nThat got my attention.\nMy first impression was that Gelman had found a new way to attack ev psych: cherry pick a few EP papers with questionable stats, and use those to tar the field.\nMost of the discussion on Gelman’s blog involved the paper by Alec Beall and Jessica Tracy, Women more likely to wear red or pink at peak fertility, published in Psychological Science, one of psychology’s top journals. The inspiration for the study was the fact that “females in many closely related species signal their fertile window in an observable manner, often involving red or pink coloration.”\n\n\n\nVictorian cartoon\n\n\nMonkey butts, menstrual cycles, sex, and the color pink. Nothing to mock there. The study, however, is more important than it sounds. If Beall and Tracy were right, human female ovulation is not concealed after all.\nIn Slate, Gelman accused Beall and Tracy of conducting a fishing expedition — repeatedly making comparisons until they found one that was “statistically significant” and then reporting it as if they had predicted it in advance — a big, though often inadvertent, no-no that he claimed was widespread in science:\n\nThere’s a larger statistical point to be made here, which is that as long as studies are conducted as fishing expeditions, with a willingness to look hard for patterns and report any comparisons that happen to be statistically significant, we will see lots of dramatic claims based on data patterns that don’t represent anything real in the general population. Again, this fishing can be done implicitly, without the researchers even realizing that they are making a series of choices enabling them to over-interpret patterns in their data.\n\nEvery scientist knows that fishing is wrong, so Beall and Tracy were understandably upset that Gelman accused them of unethical conduct without talking to them first.\nI hadn’t met either author (so far as I know), so I googled them. According to Beall’s cv, he was a grad student and this was his first, first-authored paper. What a great introduction to academia! Get published in Psych Science on your first effort, only to be slammed by a prominent statistician, repeatedly. I became a regular reader of Gelman’s blog, and a week rarely goes by that he doesn’t work in some dig at the Beall and Tracy paper. In fact, I was inspired to write this post after a recent dustup over Beall and Tracy’s refusal to share their data with Gelman since Gelman wouldn’t tell them where he was publishing his critique (they have shared their data with others).\nGelman, who presented himself as a disinterested critic who only wanted to improve science, had several criticisms of the paper, but I am going to focus on one: Gelman guessed that Beall and Tracy had originally predicted that, like female chimpanzees who signal estrus with a prominent red swelling, women would tend to choose red shirts around the time of ovulation. Gelman surmised that after collecting their data Beall and Tracy noticed that there was no statistically significant tendency to wear red shirts during the fertile period (red only: p &gt;.05), but if women wearing red shirts and women wearing pink shirts were combined into a single, new category (reddish shirts), there was a statistically significant effect (red + pink: p&lt;.05).\nI was a little confused. Researchers often combine conditions and no one blinks an eye (and Beall and Tracy deny they did this). Further, there was no evidence that Beall and Tracy had repeatedly tested various combinations of shirt colors to find one that was “significantly” associated with the fertile period of women’s menstrual cycles, and then, after the fact, had come up with some theory to explain it, which would be a classic fishing expedition.\nGelman began to walk back his accusation that Beall and Tracy went fishing. Instead, in a paper with Eric Loken in American Scientist, and on his blog, Gelman claimed that multiple comparisons can be a problem, even when the research hypothesis was posited ahead of time and researchers only conduct one statistical test.\nHuh?\nGelman’s own stats textbook didn’t mention anything about this. In fact, in the textbook and in a publication, Gelman et al. claimed “we (usually) don’t have to worry about multiple comparisons,” an irony he himself noted. Further, why pick on a grad student? Or evolutionary psychology? The problem, if there was one, would be pervasive throughout all the sciences.\nMy first impression was right. A reader of Gelman’s wondered why he spent so much time criticizing a study on pink shirts instead of the statistically flawed medical research that actually harms people, and Gelman admitted he had an agenda: surprise, surprise, he doesn’t like evolutionary psychology:\n\nBut I do think these social psychology studies make a difference too, in that they feed the idea that people are shallow and capricious, that we make all sorts of decisions based on our animal instincts etc. Sure, some of that is true but not to the extent claimed by those clueless researchers.\n\n\nTo erroneously connect fat arms [the paper co-authored by Tooby and Cosmides] or monthly cycles to political attitudes is to trivialize political attitudes, and I think that’s a mistake, whatever your politics.\n\nWell, Gelman also makes mistakes: singling out a few articles on one side of a debate that supposedly have statistical flaws, but not looking at articles on the other side of the debate, obviously says nothing about who’s right (even assuming such a debate exists; Gelman knows very little about evolutionary psychology).\nStill, if Gelman’s general points were correct, I realized I could easily be making the same mistakes in my own research, and so too would a lot of other people. With some chagrin, I had to admit that although I was aware of the many debates surrounding Frequentist vs. Bayesian approaches to statistical inference, I had never thought critically about the foundation of them both: probability.\nFollowing pointers on Gelman’s blog, I started reading. Late to the party as always, I discovered that many statisticians, such as John Ioannidis, and other quantitative types, such as Uri Simonsohn, Joseph Simmons, and Leif Nelson, believed there was a statistical crisis in science, and that it was possible that most research findings were false. Yikes! After pouring through many articles and blogs, I concluded that although the tools statisticians have given us are powerful, they are very brittle and easy to break because probability is easy to get wrong.\nThe world is noisy and we humans have a propensity to see patterns in this noise where none exist. To distinguish signal from noise, most scientists therefore rely on the following expression:\nP(D|H0)\nwhich is the probability that your data (D) would turn out the way they did given some null hypothesis (H0), such as no difference in shirt color during peak fertility. If p is small, you’re looking at a signal and not noise. Yea! This is called Null Hypothesis Significance Testing (NHST).\nThere are many critics of NHST. But the problems that Gelman and others were highlighting actually did not involve NHST per se. Instead, they were making a “garbage in, garbage out” argument. The probabilities spit out by standard software packages — p-values — were, in too many cases, misleadingly small because scientists had inadvertently fed the software the answers they wanted to see.\nProbability is a subtle concept (and I’m probably going to screw it up right now!). To illustrate the problem Gelman and others have found, I will use a casino example. Imagine that you have a die you suspect is loaded. You roll it 2 times, and it comes up 3 each time. Can you reject the null that the die is fair? You might think that because the probability of rolling one three with a fair die is 1/6, and you’ve rolled two threes, the probability under the null is \\((1/6)^2\\), which is about 0.028, so you can reject the null hypothesis that the die is fair.\nIf that’s what you thought, you would be wrong, because it’s a trick question. If you had called two threes before rolling the die, that is, you suspected it was loaded to come up threes, and then it came up all threes, you could conclude that the die is (probably) loaded. But I asked you to test the null by computing the probability of two threes based on already having rolled two threes.\nHey, you might respond, it’s odd that the die came up three both times, right? Yes it is. It would also be odd if it came up two ones, or two twos, or two fours, etc., and I gave you no reason to suspect one of these possibilities over the others. The probability that it came up two ones OR two twos OR two threes OR two fours OR two fives OR two sixes is about 0.17, i.e., not odd at all. You can’t test the null by computing a probability as if you hadn’t seen the faces of the die if your choice of probability test is based on having seen those faces. This is the core problem.\nIn science, collecting data is the analog of rolling the dice. That’s what P(D|H0) means. To use this probability to distinguish signal from noise, we scientists must therefore make a very precise prediction before collecting or looking at the data because the p-values our stat programs compute are only accurate if our “call” was not influenced by the data.\nBut we all have looked at the data. Almost every statistics textbook, including Gelman’s, recommends that we pour over our data, checking distributional assumptions and so forth, before running any test. And many common choices during data analysis, such as controlling for potential confounds, looking at interactions, and combining conditions, can dramatically increase the chance of a false positive.\nCurrent best practice in the sciences is like first rolling the dice, then meticulously examining the numbers that land face up, but promising to not let anything we learn about those numbers influence any aspect of our call.\nThink any casino would play by those rules?\nGelman and others are not criticizing NHST per se (or rather, that’s a separate argument). They are showing how easy it is to inadvertently break NHST.\nIn another irony, Gelman grounds this crisis in human nature: just like the gambler who has the strong monetary incentive to beat the house, scientists can usually only publish (and thus get credit for) “statistically significant” results, and therefore have an incentive to find some justification for altering their predictions after looking at the data (e.g., red+pink, not just red). That is, many scientists are pursuing their self-interest and taking a benefit they don’t deserve, and Gelman’s cheater detection mechanisms are on full alert. Just the phenomenon that put evolutionary psychology on the map!\nAlthough deliberate or inadvertent cheating is certainly part of the story, I want to offer a different framing: scientists currently face an untenable tradeoff between learning about the world, and confirming what they’ve learned. If Beall and Tracy had discovered, rather than predicted, that women tend to wear pink shirts and red shirts during the fertile phase of their cycle, that would be important. Discovering that you need to control for, e.g., age, would be important. After all, aren’t scientists supposed to discover things? Combining the two colors, and studying “reddish” clothing, would be excellent science. So would controlling for previously unsuspected confounds. Unfortunately, these would break the computation of statistical significance that tells us this is a real effect and not just a fluke. Good science can be bad statistics, and good statistics can be bad science.\nScience is screwed.\nThere is a way out of this mess, as statistician John Tukey realized decades ago. I’ll discuss that in my next post. For now, I will just note that there is a long history of ridiculing approaches to human sexuality that take account of our primate heritage. As targets of such ridicule, Beall and Tracy are in pretty good company:\n\nIN the discussion on Sexual Selection in my “Descent of Man,” no case interested and perplexed me so much as the brightly-coloured hinder ends and adjoining parts of certain monkeys. As these parts are more brightly coloured in one sex than the other, and as they become more brilliant during the season of love, I concluded that the colours had been gained as a sexual attraction. I was well aware that I thus laid myself open to ridicule; though in fact it is not more surprising that a monkey should display his bright-red hinder end than that a peacock should display his magnificent tail.\nCharles Darwin (1876) Sexual Selection in Relation to Monkeys. Nature, 15, 18-19."
  },
  {
    "objectID": "posts/2017-11-15-pca-new-coordinate-system-same-data/index.html",
    "href": "posts/2017-11-15-pca-new-coordinate-system-same-data/index.html",
    "title": "PCA: new coordinate system, same data",
    "section": "",
    "text": "There are many ways to teach Principal Component Analysis (PCA). This way is mine.\nThe first constellation I learned to recognize was the Big Dipper. In the evening it’s in one part of the sky, and in the early morning, another, but it’s still the Big Dipper. Same thing if I look at it while I slowly spin around. To be more specific, in each of the 3 figures below, the stars have different \\(x\\) and \\(y\\) coordinates, yet it is easy to recognize them as the same stars:\n\n\n\nBig Dipper\n\n\nThe point is: we recognize the Big Dipper, not by the specific location of the stars in the sky, but by the location of each star relative to the others.\nThe first and most important step in understanding PCA is to think about your data in the same way that you think about constellations: it’s the relationships between your data points, not their individual values, that matters.\nIn a scientific study, we typically measure multiple values on each person (or population, or whatever), e.g., age, height, weight, sex, and so forth. The mental frame shift to make is to think about the collection of multiple values on a single person as a single data point — a single “star” in the sky — and all the data points as a constellation of stars in the sky. If we had two measurements per person – e.g., height and weight – then each person (each data point) has two coordinates; if we had three measurements per person – e.g., height, weight, and age – then each person (each data point) has three coordinates, and so forth.\nIn general, you can think about each “unit” in the data – person or observation or “row” – as one point in an \\(N\\)-dimensional Euclidean space, where \\(N\\) is the number of variables that you have measured. Viewed this way, the data has a “structure” determined by the relationships of each observations to the others that will be preserved even if the coordinate system is changed.\nHere is a concrete example with 2 variables per person – height and weight – and thus a 2-dimensional space of points:\n\n# Height and weight of !Kung individuals.\n# The !Kung are an ethnic group in\n# southwest Africa.\n# From Howell via McElreath:\nd &lt;- readr::read_delim(\"https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/Howell1.csv\", delim = ';')\nlibrary(ggplot2)\nggplot(d, aes(height, weight)) + \n  geom_point(alpha = 0.5) + \n  scale_x_continuous(limits = c(0,200)) +\n  scale_y_continuous(limits = c(0,70)) +\n  geom_point(x = 0, y = 0, colour = 'red', size = 3) +\n  labs(title = \"!Kung heights and weights\", subtitle = \"Each black dot represents one person.\\nThe red dot indicates the origin of the coordinate system.\")\n\n\n\n\nMost students would (correctly) interpret this plot as depicting the relationship between height and weight.\nThere is another way.\nAlthough each star in the Big Dipper is also described by two variables – an \\(x\\) and \\(y\\) coordinate (or an ascension and declination) – both variables are in the same units (angles). This is an important reason why we can think about the Big Dipper in space without thinking about the \\(x\\) and \\(y\\) values of each star.\nThe first step on our journey is therefore to put our height and weight variables into the same units by standardizing them (subtract the mean from each variable, and then divide it by its standard deviation):\n\n# Standardize each variable\nd$zheight &lt;- scale(d$height)[,1]\nd$zweight &lt;- scale(d$weight)[,1]\n# Plot\nggplot(d, aes(zheight, zweight)) + \n  geom_point(alpha = 0.5) +\n  geom_point(x = 0, y = 0, colour = 'red', size = 3) +\n  coord_fixed(xlim = c(-4, 3), ylim = c(-3, 3)) +\n  labs(title = \"Standardized !Kung heights and weights\", subtitle = \"Each black dot represents one person.\\nThe red dot indicates the origin of the coordinate system.\")\n\n\n\n\nMany folks would (correctly) interpret these new variables as transformed versions of the original data. However, I would like you to instead see this as a transformation of the coordinate system: we have translated the origin of the coordinate system to the middle of the data (the red dot), and we have put the \\(x\\) and \\(y\\) axes on the same scale (1 unit of x equals 1 unit of y). The data remain the same.\nThe translation of the origin to the center of the data is useful because positive values on the \\(x\\)-axis now indicate values that are greater than the mean, and negative values now indicate values that are less than the mean. The same goes for the \\(y\\)-axis. The translation of the origin makes it easy to identify individuals whose heights and weights are above or below average. Transforming the coordinate system (not the data!) can help us interpret the data – the new origin has advantages over the original origin.\nPutting the \\(x\\) and \\(y\\) axes on the same scale is useful because we can now more easily think about this 2d space as a uniform height-weight space, or height-weight continuum, independent of individual \\(height\\) and \\(weight\\) values.\nWe are ready for another transformation of the coordinate system: a rotation around the origin:\n\nangle &lt;- -1.5 # angle of rotation in radians\n\n# New x and y coordinates after rotation\nd$x &lt;- d$zheight * cos(angle) - d$zweight * sin(angle)\nd$y &lt;- d$zheight * sin(angle) + d$zweight * cos(angle)\n\nggplot(d, aes(x, y)) + \n  geom_point(alpha = 0.5) +\n  geom_point(x = 0, y = 0, colour = 'red', size = 3) +\n  coord_fixed(xlim = c(-4, 3), ylim = c(-3, 3)) +\n  labs(title = \"Rotated !Kung heights and weights\", subtitle = \"Each black dot represents one person.\\nThe red dot indicates the origin of the coordinate system.\")\n\n\n\n\nHere are the pairs of coordinates of our first 6 data points in each of our 3 different coordinate systems (the original, standardized, and rotated coordinate system):\n\n\n\n\n\nheight\nweight\nzheight\nzweight\nx\ny\n\n\n\n\n151.76\n47.83\n0.49\n0.83\n0.86\n-0.43\n\n\n139.70\n36.49\n0.05\n0.06\n0.06\n-0.05\n\n\n136.52\n31.86\n-0.06\n-0.25\n-0.26\n0.04\n\n\n156.84\n53.04\n0.67\n1.18\n1.23\n-0.59\n\n\n145.42\n41.28\n0.26\n0.38\n0.40\n-0.23\n\n\n163.83\n62.99\n0.93\n1.86\n1.92\n-0.79\n\n\n\n\n\nAlthough the pairs of coordinates are radically different, we easily recognize the same constellation of data in the plots, regardless of coordinate system.\nWe have seen how translating the origin of the coordinate system to the center of the data helps us interpret the data. But how could rotating the coordinate system be helpful?\nIn most studies, we measure stuff because we know that the things we’re studying – people in this case – vary, and it is exactly this variation that we want to understand. What if we rotated the coordinate system so that the variance of the data was maximized along the \\(x\\)-axis? Then, in this rotated coordinate system, folks with large positive values on the \\(x\\)-axis would be maximally “different” from folks with large negative values on the \\(x\\)-axis in “height-weight” space. Differences in \\(y\\)-values would then be less important in distinguishing individuals.\nWe can find the rotation that maximizes variance along the \\(x\\)-axis by trial and error: simply choose different angles, compute the rotation, and then compute the variance or standard deviation along the \\(x\\)-axis. Rinse and repeat unit you find an angle that maximizes the standard deviation. There will be two such angles, each \\(\\pi\\) radians (180 degrees) apart:\n\n# Run this code over and over with\n# different values for the angle\n# until sd(d$x) is at a maximum.\nangle &lt;- -0.78 # This angle comes close; -0.78 + pi would also come close\nd$x &lt;- d$zheight * cos(angle) - d$zweight * sin(angle)\nsd(d$x) # We could also use var(d$x)\n\n[1] 1.393114\n\n\nA second way would be to use R’s optim function, which automates the above process:\n\n# This function rotates and then\n# computes sd along x.\n\nsd_x &lt;- function(angle) {\n  sd(d$zheight * cos(angle) - d$zweight * sin(angle))\n}\n\n# This function finds the angle that \n# maximizes the above function\nopt &lt;- \n  optim(\n    0, # Starting value of angle\n    sd_x, # The function to minimize\n    method = \"Brent\", # The optim procedure that works best in 1-D\n    lower = -pi,\n    upper = pi,\n    control = list(fnscale = -1) # Maximize instead of minimize\n    )\n\nAn angle of rotation (in radians) that maximizes sd along x:\nopt$par = -0.7853982\nThe maximized standard deviation:\nopt$value = 1.393134\nLet’s plot our data using that optimal angle of rotation:\n\n# one optimal angle in radians;\n# the other would be opt$par + pi\nangle &lt;- opt$par \n\n# New x and y coordinates after rotation\nd$x &lt;- d$zheight * cos(angle) - d$zweight * sin(angle)\nd$y &lt;- d$zheight * sin(angle) + d$zweight * cos(angle)\n\nggplot(d, aes(x, y)) + \n  geom_point(alpha = 0.5) +\n  geom_point(x = 0, y = 0, colour = 'red', size = 3) +\n  coord_fixed(xlim = c(-4, 3), ylim = c(-3, 3)) +\n  labs(title = \"!Kung heights and weights rotated to maximize variance along x-axis\", subtitle = \"Each black dot represents one person.\\nThe red dot indicates the origin of the coordinate system.\")\n\n\n\n\nGuess what? The \\(x\\)-axis is principal component 1 (PC1), and the \\(y\\)-axis is principle component 2 (PC2), as we can confirm by comparing our results to those from R’s prcomp (principal component) function:\n\n# Compute PCA using the standard R function\nm &lt;- prcomp(~ zheight + zweight, data = d)\nsummary(m)\n\nImportance of components:\n                          PC1     PC2\nStandard deviation     1.3931 0.24326\nProportion of Variance 0.9704 0.02959\nCumulative Proportion  0.9704 1.00000\n\n# Compare the standard deviations above with:\nsd(d$x)\n\n[1] 1.393134\n\nsd(d$y)\n\n[1] 0.2432649\n\n\nCompare our x & y values…\n\n\n\nx\ny\n\n\n\n\n0.9326787\n0.2409332\n\n\n0.0788411\n0.0052468\n\n\n-0.2244852\n-0.1354080\n\n\n1.3134063\n0.3613867\n\n\n0.4554072\n0.0890045\n\n\n1.9703735\n0.6604768\n\n\n\n… with those from prcomp\n\n\n\nPC1\nPC2\n\n\n\n\n-0.9326787\n-0.2409332\n\n\n-0.0788411\n-0.0052468\n\n\n0.2244852\n0.1354080\n\n\n-1.3134063\n-0.3613867\n\n\n-0.4554072\n-0.0890045\n\n\n-1.9703735\n-0.6604768\n\n\n\nThe minus signs are reversed because the axes are rotated 180 degrees, but the variance is still maximized along the x-axis (remember, there are 2 rotations that will maximize the variance).\nIn summary, principal components are simply a new orthogonal (perpendicular) coordinate system for your data, rotated so the variance of your data is maximized along the first axis (PC1); then, rotating around the first axis, the remaining variance is maximized along the second axis (PC2), which is perpendicular to the first; and so forth, until the directions of all axes are specified. Thus, there will be as many principal components as there are dimensions in your data (i.e., number of variables), and the variance will decrease across each successive component across each successive component.\nThere are many uses of this new coordinate system. In our example, 97% of the variance in our data falls along PC1. Thus, we might interpret PC1, which is a combination of height and weight, as something like size. By rotating our coordinate system, we have identified underlying “structure” in our data. For 2-d data like our example, PCA is not that useful. But when our data have many dimensions, PCA and related techniques can find structure that would be difficult or impossible to find without them.\n\nNote #1: The prcomp and other PCA functions do not find these rotations in the same way we did. Instead, they use methods like singular value decomposition, which you can read about on wikipedia.\nNote #2: You might have heard of rotation after PCA, or terms like varimax rotation. These also seek useful rotations, but are distinct from PCA. You can read more about them, and their relationship to PCA here, here, and here.\nNote #3: There is a great set of alternative explanations of PCA here ."
  },
  {
    "objectID": "posts/2020-01-21-is-evolutionary-psychology-impossible/index.html",
    "href": "posts/2020-01-21-is-evolutionary-psychology-impossible/index.html",
    "title": "Is evolutionary psychology impossible?",
    "section": "",
    "text": "Subrena Smith recently argued that “evolutionary psychology, as it is currently understood, is…impossible” (Smith 2019). I agree with most of Smith’s premises, which are based on the logic of evolution by natural selection, and I think other evolutionary psychologists would too. So why does Smith conclude that evolutionary psychology (EP) is impossible but I conclude the opposite? Well, actually, Smith provides an example of evolved psychology that she considers “highly plausible,” directly contradicting her strong conclusion that EP is impossible. As I will show, this highly plausible example shares essential features with broad swaths of research in EP.\nSmith does a decent job capturing the core tenets of EP: much like the rest of the body, the brain comprises many distinct functional components, termed psychological adaptations. These evolved by natural selection to solve particular computational problems related to survival and reproduction in ancestral environments, otherwise known as the environment of evolutionary adaptedness (EEA).\nAccording to EP, evidence for the existence of a psychological adaptation includes (1) evidence of a computational problem posed by the ancestral environment (e.g., recognizing different types of foods and other objects); (2) evidence or argument that solving this computational problem would have increased the biological fitness of ancestors with the trait relative to those without it (e.g., by enabling them to find more food); (3) evidence that humans have the computational ability to solve precisely this problem (e.g., are able to rapidly recognize a large number of objects), which would typically include details about the specific algorithms involved; and (4) evidence that the computational ability reliably develops in (almost) all humans (of at least one sex), in all environments that do not deviate too much from the EEA. Together, these are taken as evidence of design – qualities that have been recognized since antiquity to distinguish “beneficial” organism traits from other natural phenomena (Aristotle, Physics II 8), and for which Darwin’s theory of natural selection provides the modern scientific explanation (Darwin 1859; Williams 1966).\nVision and hearing are uncontroversial examples of psychological adaptations, whereas mate preferences that differ between the sexes are somewhat controversial examples.\nSmith’s main critique of EP involves what she calls the “matching problem.” For a cognitive trait such as object recognition to be an adaptation, it must be the case that not only does this trait reliably develop in modern humans, but it must have reliably developed in ancestral humans too, which Smith terms “strong vertical homology”:\nI agree, and I think most evolutionary psychologists would too. In essence, Smith has added more requirements to the already long list that distinguish adaptations from other traits.\nSmith then identifies the “matching problem”: how can we know that ancestral humans had the cognitive trait in question? After all, cognitive traits don’t fossilize. Furthermore, complex cognitive abilities can be acquired via learning. As one example, Smith points to reading.\nEP would exclude reading cognition as a possible psychological adaptation because reading abilities do not reliably develop in all humans – many populations are non-literate, and many individuals within populations can fail to learn how to read. Still, it’s not hard to imagine an alternate universe where EP emerges as a discipline after literacy had become essentially universal and the existence of non-literate populations had been forgotten. Reading would then satisfy all three criteria above: it’s a complex cognitive ability, it provides many benefits, and (in the alternate universe) it reliably develops in essentially all modern humans. Yet the inference that reading is an adaptation would be incorrect. As Smith argues:\nAt first, it seems like the matching problem is insurmountable. How can we possibly know the mental structures of ancestors living hundreds of thousands or millions of years ago?\nIn the face of such a barrier to scientific investigation, Smith seems to favor what she views as the evolutionary alternative to EP, that “evolution fashioned the human mind as a domain-general or modestly modular learning system.” This leaves the impression that because (in her view) EP hypotheses cannot be tested, they are therefore false, and that because (in her view) a domain-general learning system can be tested, it is more likely to be true. No such inference is possible, of course. At best, based on her analysis, we would have to admit that EP hypotheses could be true but we can’t collect some of the evidence needed to test them.\nFortunately, Smith herself provides a solution to the matching problem:\nSo, it turns out that ordinary homology provides convincing evidence for strong vertical homology, and thus solves the matching problem.\nAlthough not acknowledged by Smith, evolutionary psychologists emphasize the importance of comparative data, including both homologies and analogies (see Figure 1).\nFigure 1: Schematic representation of the different forms of evidence used to evaluate the validity of psychological adaptations. AI: artificial intelligence. Figure and caption from Schmitt and Pilcher (2004).\nBroad swaths of evolutionary psychology draw on homologies between human psychology and the psychology of our primate and mammalian relatives. There is even an Oxford Handbook of Comparative Evolutionary Psychology (Vonk & Shackelford 2012).\nFigure 2: Oxford Handbook of Comparative Evolutionary Psychology, Vonk & Shackelford, Eds. (2012).\nExamples of comparative evolutionary psychology include spatial memory (Haun et al. 2006), pathogen avoidance (Schaller 2015), attachment and maternal care (Maestripieri & Roney 2006), the expression of emotions (Darwin 1872), prosociality (Silk and House 2012), the biological roots of music (Hagen and Hammerstein, 2009), and social learning (Whiten 2017). To the degree that the evidence supports homology (and can rule out analogy), we can be confident that human ancestors possessed the trait in question.\nKeep in mind that Smith is arguing that EP is impossible in principle. The strength or weakness of the empirical evidence in each of the above examples, which varies quite a bit, is therefore not at issue. At issue is whether it is possible to provide convincing evidence of homology, and thus that human ancestors possessed the cognitive mechanisms in question. The eye-blink and other examples make clear that it is, and that evolutionary psychologists are attuned to the theoretical importance of homologies between psychological traits in humans and those in our relatives. By Smith’s own criteria, much EP is possible.\nPutative psychological adaptations that are unique to the human lineage, however, present difficulties. One can no longer invoke homology as evidence that cognitive structures that reliably develop in modern humans also did so in ancestral humans. Language is an ideal example because language abilities are not present in chimpanzees or other primate relatives. Indeed, although some evolutionary psychologists argue that language has all the hallmarks of adaptation, such as complex cognitive design, fitness benefits, and reliable development in all humans (e.g., Pinker and Bloom 1990; Pinker and Jackendoff 2005), other evolutionary and cognitive researchers argue that there was no selection for language specifically and that it instead emerges, e.g., as a byproduct of cognitive abilities such as recursion (Hauser et al. 2002) or from learning and other cognitive biases and coordination with others (Christiansen and Chater 2008).\nAs Smith repeatedly notes, psychological adaptations, like other adaptations, are inherited genetically. She fails to acknowledge, though, that we now have the complete sequence of the human genome, and the genome contains information on human-specific positive selection (I blog about genetic evolution in the human lineage here). We do not yet have the knowledge to link the development of most human adaptations, psychological or physiological, to specific sequences in the genome. But when it comes to language, there is progress. Rare mutations in FOXP2, a transcription factor, appear to disrupt language development, suggesting that FOXP2 plays a critical role in language. There is evidence that transcriptional enhancers in the FOXP2 locus underwent accelerated evolution in the human lineage (Caporale et al. 2019).\nAt this point, such evidence is suggestive at best. Genes have multiple effects and it could be that positive selection on FOXP2 regulation was due to non-language or non-cognitive effects of FOXP2. It could also be that FOXP2 influences language via, e.g., its effects on aspects of cognition that are not specific to language, such as recursion, and it was these abilities that underwent recent positive selection. Still, these results indicate that in coming years it might be possible to test EP and other adaptationist hypotheses using genetic data.\nBut even without genetic data, it is possible to test adaptationist hypotheses about species-specific adaptations. Imagine, for instance, that species X, which is living in an environment disturbed by recent human activity, has a complex soft-tissue trait that reliably develops in all members of the species. This trait doesn’t fossilize, it does not appear in any other species, and its genetic basis is unknown. It could be that this trait is a non-functional developmental consequence of novel environmental factors – perhaps chemical pollutants. But it could be an adaptation. Research on the trait, its relationship to what we know about the species’ ancestral environment, how the species makes a living, and on the underlying developmental mechanisms, as well as theoretical advances, can either indicate design or the lack thereof, and thus add weight to an adaptationist or non-adaptationist hypothesis, respectively. This approach characterizes research on menopause, which only occurs in humans and four species of toothed whales, and for which there are many competing hypotheses involving adaptation vs. byproduct of senescence (e.g., Kirkwood & Shanley 2010; Johnstone & Cant 2019). Yet according to Smith, in such cases scientists should just throw up their hands.\nSmith’s view of science is myopic. Abduction, i.e., inference to the best explanation, is the cornerstone of scientific methodology (Douven 2017). Scientific hypotheses compete with one another to provide the “best” explanation of some phenomenon, where “best” typically involves criteria like accurately predicting new and surprising observations, parsimony, and coherence. Abduction specifically does not require that scientists produce direct evidence for every single entailment of the hypothesis. The eye-blink adaptation hypothesis entails a genetic basis for the reflex, yet Smith is willing to accept the hypothesis without direct evidence for eye-blink genes. “Strong vertical homology” is just another of the many criteria any hypothesis of complex adaptation must meet, and not the most important one (that honor belongs to design). And, contra Smith, more such criteria make a theory more testable, not less, because there are now more ways to falsify it. EP hypotheses make many unique predictions that are testable in living humans, and are thus able to compete with other hypotheses to explain language and other cognitive phenomena.\nSmith, nevertheless, is right to draw attention to the importance of homology and the comparative method in testing adaptationist hypotheses. And to be fair, EP probably under-utilizes this powerful tool. She is wrong to ignore the many EP studies that do employ the comparative method, however, and she incorrectly concludes that if evidence for one of the many predictions of an adaptationist hypothesis is currently missing (e.g., evidence of “strong vertical homology”) it is therefore impossible to test that hypothesis against competing hypotheses.\nMany thanks to Laith Al-Shawaf for helpful comments on an earlier draft.\n2020/01/25: Added refs for menopause example."
  },
  {
    "objectID": "posts/2020-01-21-is-evolutionary-psychology-impossible/index.html#footnotes",
    "href": "posts/2020-01-21-is-evolutionary-psychology-impossible/index.html#footnotes",
    "title": "Is evolutionary psychology impossible?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSmith provides additional lines of evidence for an eye-blink adaptation that are either the same as standard adaptationist arguments and/or do not address the matching problem: “Second, it is clear that the eye-blink reflex protects the eye from injury in the taxa where it is found. Third, it is clear that the fitness of organisms that rely on vision would be impeded if there were not some mechanism for protecting the eye from injury. Fourth, the physiological mechanism underpinning the reflex is well understood. Fifth and finally, the reflex operates automatically, and is therefore mandatory. It is not “up to” the organism whose reflex it is. These five factors, taken together, support the claim that the eye-blink reflex in contemporary humans is strongly vertically homologous to the eye-blink reflex in earlier members of the lineage.”↩︎"
  },
  {
    "objectID": "posts/2018-03-03-while-it-may-be-true-that-evolutionary-anthropologists-consider-themselves-scientists-and-use-the-terms-evolution-and-evolutionary/index.html",
    "href": "posts/2018-03-03-while-it-may-be-true-that-evolutionary-anthropologists-consider-themselves-scientists-and-use-the-terms-evolution-and-evolutionary/index.html",
    "title": "While it may be true that Evolutionary Anthropologists consider themselves scientists…",
    "section": "",
    "text": "Casey Roulette, a former PhD student of mine who is now an assistant professor at San Diego State University, recently received an email from a member of the Biology Department who was irate that Casey’s evolutionary anthropology course, Evolution of Human Nature, was being considered to fulfill “Natural Sciences” GE reqs. She informed him that she had complained to her Chair, who in turn complained to the Dean of the College of Sciences, and that “Both will be preparing a letter to be sent to the GE committee indicating that the College of Sciences does not support this course as a GE course.” The email concluded:\nShe attached an evaluation from the SDSU Department of Biology. The first part argued that, on programmatic grounds, the course did a good job fulfilling the Social Science reqs. I agree.\nThe second part, however, echoed the email, arguing on scientific grounds that the course did not qualify as a GE course in the Natural Sciences because it presented a view of biology that was incomplete, biased and incorrect.\nThat’s odd. If true, Casey’s course shouldn’t qualify for GE in either the Social or Natural Sciences. In fact, his course shouldn’t be taught at all.\nSDSU biologists’ first concern was that Casey presents evolution as synonymous with adaptation:\nTheir second concern was that Casey does not discuss genetic drift, and its role in human evolution:\nBy “the pan selectionist approach” I suppose they are referring to the focus of Casey’s course, Human Behavioral Ecology (HBE).\nThe debate over adaptationism, however, is an ongoing debate, one that began more than a century ago. Each side has involved towering figures in evolutionary biology like Fisher, Wright, Williams, Hamilton, Maynard Smith, Gould, Lewontin, and Kimura.\nThe neutralist–selectionist debate — are patterns of genetic variation primarily explained by random genetic drift or natural selection? — is, again, a debate.\nDoes Casey’s syllabus present both sides of these debates?\nThe assigned reading in Week 2 of the course is Stephen Jay Gould’s Sociobiology: the art of storytelling. It was Gould, of course, who with co-author Richard Lewontin, introduced the phrase “Adaptationist Programme” in their hugely influential article The spandrels of San Marco and the Panglossian paradigm: a critique of the adaptationist programme. One of Gould’s key points is the important role of genetic drift.\nMore importantly, one of two required books for the course is Laland and Brown’s Sense and nonsense: Evolutionary perspectives on human behaviour. I would count Laland as a consistent critic of the adaptationist program (in favor of his alternative, niche construction, developed in collaboration with eminent population geneticist Marcus Feldman) whose book is a good-faith effort to describe the controversies as they apply to human evolution. Laland and Brown do discuss neutral theory, drift and molecular evolution, especially their intriguing parallels with cultural evolution. Casey also assigned chapter 7 of Evolution of Human Behavior, by Agustin Fuentes, who I would also count as a critic of adaptationism.\nAlthough Casey’s course focuses on the evolution of the human behavioral phenotype and not molecular evolution, I see no evidence that the course, which also assigns Wrangham’s Demonic males, is giving students a biased overview of the debates over adaptationism and neutralism vs. selectionism.\nAnd it’s not quite true that neutral models “have been the dominant models of molecular evolution for 50 years.” Instead, the intense debate has perhaps resulted in a consensus. According to evolutionary geneticists Charlesworth and Charlesworth, “From the late 1980s, the neutral theory came increasingly to be used as a null hypothesis, against which alternative hypotheses could be tested, including the models of the effects of selection on neutral or nearly neutral variability at linked sites….”, a point also made by Masatoshi Nei and Kimura himself.\nAt this point I should admit that I’m an unreconstructed ultra-Darwinian Fundamentalist who, each night, reads his daughters passages from the Selfish Gene. Depression is an adaptation! Drug use is an adaptation! This was my logo for the first iteration of this blog:\nCasey, what’s with the balanced overview? Did I teach you nothing?\nIn my view, critics of adaptationism have things backwards. For adaptationists, the question is not, do constraints and noise play an important role in organism structure? The question is, why don’t constraints and noise dominate organism structure? Constraints and noise permeate physical processes, yet organisms — intricate machines that surpass all human technology — somehow manage to make precise copies of themselves in hostile environments. It is exactly this problem, as I discuss in a bit more detail here, that adaptationists are trying to solve.\nThe population genetics folks have it right: random genetic variation, and more generally, noise, by-products, constraints, and thermodynamically favored physics and chemistry, should always be the null hypotheses that an adaptationist hypothesis needs to beat.\nCourse approval at SDSU is a pretty trivial topic, but the evidence for noise vs. selection in the human genome is not. I therefore thought I would use this post as an opportunity to learn a bit more about it. It’s not meant to be a comprehensive review, just a taste of some major issues. I’m not a genetics guy — far from it — so if I make any mistakes, let me know in the comments."
  },
  {
    "objectID": "posts/2018-03-03-while-it-may-be-true-that-evolutionary-anthropologists-consider-themselves-scientists-and-use-the-terms-evolution-and-evolutionary/index.html#empirical-evidence-that-links-genetic-evolution-with-phenotype-evolution",
    "href": "posts/2018-03-03-while-it-may-be-true-that-evolutionary-anthropologists-consider-themselves-scientists-and-use-the-terms-evolution-and-evolutionary/index.html#empirical-evidence-that-links-genetic-evolution-with-phenotype-evolution",
    "title": "While it may be true that Evolutionary Anthropologists consider themselves scientists…",
    "section": "Empirical evidence that links genetic evolution with phenotype evolution",
    "text": "Empirical evidence that links genetic evolution with phenotype evolution\nWe are only at the beginning of very long quest to link phenotypes, including the evolved behavioral and cultural phenotypes that are the topic of Casey’s course, to the genome, the focus of the SDSU biologists’ letter. Nevertheless, we know a lot more today than we knew 10 or even 5 years ago.\n\nFunctional vs. junk DNA\nGenomes can be divided into two parts: functional DNA, which plays a profound role in the phenotype, and “junk” DNA, which plays no role in the phenotype. Functional DNA comprises protein coding regions and non-coding regulatory regions. We now have a pretty good understanding of which DNA sequences code for protein. It is much more difficult, however, to distinguish regulatory DNA from “junk” DNA, and hence to determine the fraction of the genome that is functional, and therefore subject to evolution by natural selectione.\nMost attempts to distinguish functional from junk DNA involve identifying sequences that are conserved across species, and are therefore presumed to be under purifying selection (constrained sequences) and thus functional, vs. those that are not conserved across species and so presumably have not been under purifying selection and are therefore likely non-functional “junk” (c.f., ENCODE).\nA recent estimate (Rands et al. 2014) is that 8.2% (7.1–9.2%) of the human genome is functional (constrained). Protein coding sequences comprise about 1% of the genome, and are highly conserved across the mammals (Figure 11, red). Non-coding regulatory sequences have much higher rates of turnover (turnover refers to the loss or gain of purifying selection at a particular locus of the genome caused by changes in the physical or genetic environment, or mutations at the locus itself, that switch it from being functional to being non-functional or vice versa). See Figure 11:\n\n\n\n\n\nFigure 11: Schematic summary of the fraction of constrained sequence that has been retained (saturated colours) or turned over (pastel colours) in the human lineage over time (X-axis, divergence time) and how it has been distributed across various categories of functional element. In addition to showing the reduced quantity of preserved constrained sequence with increasing divergence, we infer the reciprocal quantity of sequence that is assumed to have been gained over human lineage evolution. Figure and caption from Rands et al. 2014.\n\n\n\n\nBecause \\(&gt;90\\%\\) of the human genome appears to be non-functional, sequence variation in this portion should closely follow the neutral model. Our focus, then, is on the evolution of the \\(\\sim 8\\%\\) of the genome that is functional.\n\n\nConserved traits\nPerhaps the most important point is that evolutionary anthropologists are keenly interested in the adaptations we share with primates, mammals, vertebrates, and so on. Examples include lacation, the immune system, vision and bitter taste and other plant toxin defense mechanisms that are central to Casey’s research. The major features of these adaptations evolved long before the appearance of Homo, with its small \\(N_e\\), but would need to have been maintained by purifying selection during the evolution of Homo.\nMuch of the genetic basis of adaptations we share with other mammals and primates almost certainly lies in the \\(\\sim 2\\%\\) of constrained sequence we share with all other mammals and the \\(\\sim 6-7\\%\\) we share with other primates (out of 8.2% total; Figure 11).\nMoreover, many complex adaptations have evolved features that enable them to adjust ontogenetically to local environmental conditions, an ability that I and many others have framed in strategic terms. Examples include the immune system and induction of xenobiotic metabolizing enzymes. Humans would not be doing too horribly in a wide range of environments even if there were no human-specific adaptations.\n\n\nHuman-specific selection in regulatory sequences\nDespite the low \\(N_e\\) in Homo, there is increasing genetic evidence for positive natural selection in the human lineage since our divergence from chimpanzees. Given that protein-coding sequences are highly conserved across the mammals, and that most functional DNA comprises regulatory sequences, most human-specific adaptations should be grounded in changes to regulatory sequences.\nHuman accelerated regions (HARs) are short, evolutionarily conserved DNA sequences that have acquired significantly more DNA substitutions than expected in the human lineage since divergence from chimpanzees. HARs are often, but not always, the product of positive natural selection (other mechanisms include relaxation of constraint, which allows a region to acquire more mutations than it would under purifying selection, and GC-biased gene conversion; Franchini and Pollard 2017).\nFranchini and Pollard 2017 summarized multiple studies that attempted to discover HARs, which differed in the number of species considered to determine “conserved” (which ranged from a few primate species to multiple primate, mammalian and vertebrate species), sequence filtering criteria (e.g., including or excluding coding sequences), and statistical tests for acceleration. Although each study identified a large number of HARs, there was only limited overlap in the identified regions. See Figure 12:\n\n\n\n\n\nFigure 12: Identification of human accelerated elements. Top: the four different approaches used to identify human accelerated regions. Some key differences include (i) the conserved elements used as candidates to identify HARs (which depend on multiple sequence alignments, methods to detect conservation, and whether human was masked in the alignments), (ii) bioinformatics filters that aim to restrict to non-coding elements and/or remove assembly or alignment artifacts, and (iii) tests used to detect acceleration. Bottom: overlap of the different datasets of human accelerated regions. Abbreviations: ANC accelerated conserved non-coding sequences [20]; HACNS human accelerated conserved non-coding sequences [23]; HTBE human terminal branch elements [21]. HARs include the original HARs [19] and the second generation HARs or 2xHARs [100]. Figure and caption from Franchini and Pollard 2017.\n\n\n\n\nMost studies of HARs used only sequence data. Some recent studies, though, also incorporated expression data. Gittelman et al. 2015, for example, used maps of DNase I hypersensitive sites (DHSs) from ENCODE and the Roadmap Epigenomics Projects. DHSs are regions of chromatin that serve as markers of regulatory DNA. Of 2,093,197 DHS loci, 113,577 exhibited significant constraint across the primates. DHSs active in fetal cell types, especially fetal brain cells, showed the highest levels of conservation in non-human primates. Of the conserved loci, 524 were accelerated in human evolution (haDHSs), evolving at approximately four times the neutral rate in the human lineage, mostly but not exclusively under positive selection, while other primate lineages evolved at less than half of the neutral rate. Gittelman et al. found that haDHSs tend to target developmentally and neuronally important genes relative to conserved DHSs, which themselves are already highly enriched for these categories.\nIn another review of several previous studies of HARs, Levchenko et al. 2018 noted that of the 3500 candidates identified so far, most are in non-coding regions, estimates of the fraction under positive selection range from 15%-85%, many are active in the brain, consistent with this major phenotypic difference between humans and chimpanzees, and about 7-8% are not shared with Neanderthals or Denisovans, consistent with the time of their divergence from the modern human lineage.\n\n\nRecent human evolution\nHuman \\(N_e\\) started a dramatic increase some 40-50,000 years ago as humans entered multiple new environments. Hawks et al. (2007) predicted and found that “selection has accelerated greatly during the last 40,000 years.” Co-authors Cochran and Harpending followed up with a book, The 10,000 year explosion: How civilization accelerated human evolution (you can find my review of it here). Fan et al. (2016) review many examples of recent, population-specific human adaptations to local environments, such as the arctic, tropical rainforests, and high altitude:\n\n\n\n\n\nFigure 13: Examples of human local adaptations, each labeled by the phenotype and/or selection pressure, and the genetic loci under selection. Figure and caption from Fan et al. (2016)."
  },
  {
    "objectID": "posts/2018-03-03-while-it-may-be-true-that-evolutionary-anthropologists-consider-themselves-scientists-and-use-the-terms-evolution-and-evolutionary/index.html#wrap-up",
    "href": "posts/2018-03-03-while-it-may-be-true-that-evolutionary-anthropologists-consider-themselves-scientists-and-use-the-terms-evolution-and-evolutionary/index.html#wrap-up",
    "title": "While it may be true that Evolutionary Anthropologists consider themselves scientists…",
    "section": "Wrap up",
    "text": "Wrap up\nEvolutionary anthropologists and human behavioral ecologists investigate the relationships between human environments and humans’ intricate molecular, cellular, anatomical, and behavioral phenotypes, most of which we inherited from primate, mammalian and earlier ancestors that evolved long before our lineage experienced a low \\(N_e\\). The extent to which a small \\(N_e\\) limited human-specific evolution is still under investigation. It certainly didn’t reduce it to zero, and there are many other factors that might have accelerated it. There is abundant phenotypic evidence for human-specific adaptations, after all, and there is increasing genetic evidence that positive selection played an important role in our evolutionary history, especially selection on standing variation, which is not limited by low \\(N_e\\).\nReading through the papers I discussed here, I was encouraged that folks studying human genetic variation seemed interested in, not hostile to, the many adaptationist hypotheses put forward by evolutionary anthropologists and behavioral ecologists. Charlesworth and Charlesworth, for instance, highlight the importance of kin selection and evolutionary game theory, two theoretical foundations of behavioral ecology. O’Bleness et al. (2012) give a nod to Lieberman’s endurance running hypothesizes and many other phenotypic comparisons of humans and non-human primates.\nSo what’s up with the SDSU biologists? The debates over adaptationism and neutralism vs. selectionism have all the hallmarks of a sectarian conflict. Contrary to our rhetoric, academics are among the most ethnocentric of an infamously ethnocentric species. Perhaps — dare I say it? — ethnocentrism is part of human nature. If so, it’s true of us all.\nBut, among the primates, our species also evolved a unique ability for group alliances. The SDSU biologists are doing cool stuff. Casey is doing cool stuff. Knowing Casey as I do, he doesn’t need my advice, but I’ll give it anyway: I doubt the SDSU biologists speak with one voice. There is as much diversity of opinion within groups as there is between them, maybe more. You are one of the very few SDSU social scientists who has a professional interest in what the SDSU biologists are doing. I suspect many of them recognize that.\n2018/7/15: minor edits to this post to improve clarity."
  },
  {
    "objectID": "posts/2018-03-03-while-it-may-be-true-that-evolutionary-anthropologists-consider-themselves-scientists-and-use-the-terms-evolution-and-evolutionary/index.html#footnotes",
    "href": "posts/2018-03-03-while-it-may-be-true-that-evolutionary-anthropologists-consider-themselves-scientists-and-use-the-terms-evolution-and-evolutionary/index.html#footnotes",
    "title": "While it may be true that Evolutionary Anthropologists consider themselves scientists…",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere is debate about the human mutation rate. See Scally and Durbin 2012 and Scally 2016.↩︎"
  },
  {
    "objectID": "posts/2019-03-21-measles-mothers-leadership-and-the-evolution-of-big-brains/index.html",
    "href": "posts/2019-03-21-measles-mothers-leadership-and-the-evolution-of-big-brains/index.html",
    "title": "Measles, mothers, leadership, and the evolution of big brains",
    "section": "",
    "text": "Nancy Pelosi recently won a major conflict with Trump over border wall funding, and her victory flipped the media narrative. Instead of the insider who was out-of-step with both mainstream Americans and the progressive wing of her party, Pelosi was now seen as a masterful leader. The source of Pelosi’s moxie, according to many articles that appeared at the time, is that she’s a mother of five, and the daughter of another powerful women who raised six kids. Mothering and leadership, it seems, go together. What’s the link?\nMy grad student Zach Garfield and I believe we might have the answer. Despite the fact that almost all leaders in all societies are men, natural selection for leadership abilities might have been strongest in women, at least initially (preprint). I started thinking about this problem almost 20 years ago when the New Yorker and W.W. Norton published an article and book by Patrick Tierney that tried to profit from a measles vaccine scare, similar to the efforts of Andrew Wakefield and Jenny McCarthy (the latter two have been depressingly successful – there is a measles outbreak near our campus in southwest Washington).\nHere’s the story of how an attempt to make a buck off a vaccine scare unearthed an important but virtually unknown theory by one of the fathers of American genetics, James Neel.\nLate in the summer of 2000, an email spread like, well, measles, through the anthropological community. Sent by two credulous anthropologists, it advertised an upcoming book and New Yorker article by Patrick Tierney that accused James Neel, an acclaimed geneticist, and Nap Chagnon, a well-known but controversial anthropologist, of deliberately administering a dangerous measles vaccine to native South Americans. This vaccine could supposedly cause or exacerbate a deadly measles epidemic, allowing Neel and Chagnon to study the effects of the disease in an indigenous population so as to test Neel’s sinister eugenics theory.\nPrior to this I was vaguely aware of Neel’s work on the genetics of Native South Americans. Chagnon, on the other hand, I knew well. He was my MA thesis adviser. Our department chair therefore asked me and Michael Price, another grad student who had worked with Chagnon, to investigate. Along with John Tooby, a colleague of Chagnon (and chair of my PhD committee), we starting retrieving all the documents cited in Tierney’s meticulously referenced book.\nThe vaccine story crumbled almost immediately. Tierney had deliberately misquoted all of his sources, each and every one of which documented the safety and effectiveness of the vaccine. You can read our report here. Neel and Chagnon’s vaccination program probably saved hundreds of lives.\nBut Neel’s sinister theory intrigued me. It had something to do with the evolution of human intelligence, but I had never seen it cited in any of the countless papers on the topic. Once I got hold of Neel’s publications and figured out what he was actually saying, as opposed to Tierney’s mangled version, I realized it was a damn good idea. Neel’s research in the early 1960’s had found that in Amazonian populations, headmen had more wives and children than other men, a pattern that has now been seen in many other populations. Neel reasoned that if this pattern characterized humans societies during our evolution, there would have been tremendous sexual selection for whatever trait(s) predisposed men to become leaders.\nSexual selection often results in exaggerated traits. In gorillas, for example, a single adult male typically has a harem of several females. This means that several other adult males do not have mates. Males therefore physically compete with other males for access to females. As a consequence, male gorillas are about twice as large as female gorillas, and have much larger canine teeth. Chimpanzee males are modesty larger than females, and also have much larger canines (Plavcan 2001).\nHuman males, on the other hand, are only 15% larger than females and their canines are only slightly larger (Plavcan 2001). What human trait might have become exaggerated due to sexual selection operating on headmen and other leaders? Human brains are about 3 times bigger than chimp or gorilla brains. Neel proposed that headmen become headmen because they’re smart. The dramatic increase in brain size in humans compared to other apes, according to Neel, was due to sexual selection for intelligent leaders.\nNeel only briefly sketched his theory. He didn’t explain how intelligence predisposes men to become leaders, or why leaders would attract more mates than other men.\nI gave these problems a bit of thought. First, what defines a “leader”? It seemed to me that leaders in small-scale societies are those that develop a reputation for making good decisions for the group. But why did this require exceptional intelligence? Making a decision that is good for oneself involves finding the option that maximizes an individual payoff. Making a decision that is good for the group, though, would involve searching over combinations of everyone’s options and payoffs. This could result in combinatorial explosion, a problem that might require a substantial increase in computational resources, i.e., a much bigger brain.\nThere was a problem with Neel’s theory, though. It seemed to predict that, just like the sex difference in body size in gorillas, there should be a sex difference in human intelligence, and there isn’t. I decided to put this project aside for awhile, but resolved to pick it up again at some point in the future.\nFast forward a decade. My new grad student Zach Garfield took up the project and dug into the ethnographic and theoretical literature on leadership. Leaders in traditional societies were indeed often seen as knowledgeable and intelligent. Existing evolutionary theories of leadership, though, could either account for leaders attracting followers, or leaders attracting mates, but not both.\nOur key insight came from old-school anthropology. Human societies have a nested social structure. Human families are nested within residential groups (e.g., hunter-gatherer bands), which are nested within larger, multi-community alliances and ethnic groups:\n\n\n\n\n\nTypical social structure of a hunter-gatherer society. An adult male and female cooperate to raise their children in a family. A small number of families coooperate within a band to hunt and gather food and raise their children. Multiple bands form an alliance to buffer variation in food and water supplies, and to defend territory. Figure modified from Parker et al. 2002.\n\n\n\n\nThis social structure seems to derive from several traits that distinguished ancestral humans from chimps and other apes. In hunter-gatherer societies, males and females typically form long-term pair-bonds to raise their joint offspring in families. These families cooperate to raise their kids (alloparenting), and to hunt and gather food in a band. Bands cooperate to buffer variation in access to resources and to defend territory.\nNotice that in Figure @ref(fig:nested), half the group members are kids. Humans, unlike chimps and gorillas, have short interbirth intervals combined with an exceptionally long juvenile period. In natural fertility populations (no modern birth control), women typically have 5 or 6 six kids or more, all of whom will simultaneously depend on her for up to twenty years. Although there is a high child mortality rate, the majority of family members, and therefore group members, are usually kids, ranging from infants to teenagers. And who is making good decisions for this motley crew, day in, day out, morning to night? Their mothers.\nZach and I realized that mothers might be the archetypal leaders in ancestral human societies. Raising human children involves twenty years or more of cooperation between the mother and father. Given the nested social structure of human societies, the cognitive abilities that would be required to lead the family would also be required to lead the band. Almost all the literature on leadership has remarked on the huge male bias in leadership, completely missing the fact that in every society women, just like Nancy Pelosi, routinely lead their families.\nIn summary, Zach and I propose that during human evolution there was sexual selection on both males and females for cognitive traits that resulted in high-quality decision making for the group, be it the family, band, or larger political group. Men preferred women as mates who displayed evidence of high quality decision-making because their families would do better, and women preferred men who displayed the same. Women, who in natural fertility populations are giving birth every few years, would primarily lead at the family level, but would often transition to community leadership roles when they were older. Men with exceptional decision-making skills would rise to leadership positions in the community. Zach, in his research on leadership among the Chabu (a recently settled group of former hunter-gatherers), has found that men and women both occupy community leadership roles. Consistent with our reworking of Neel’s theory, male and female leaders tend to be married to each other.\nThis is the abstract of our paper (preprint here). Comments and criticisms welcome!\n\nLong before the term Machiavellian Intelligence was coined, James Neel was pondering the role of ‘princes’ in the evolution of exceptional human intelligence. The two cornerstones of Neel’s theory – leaders’ superior skills, knowledge, and intelligence (at least as judged by others), and their greater reproductive success – have been amply confirmed by subsequent research. Neither Neel nor later theorists, however, have adequately explained why knowledgeable, intelligent leaders are attractive both to followers and to mates. We aim to fill this gap by operationalizing leaders as individuals who regularly make decisions that benefit most members of the group. Because human nuclear families comprise two unrelated individuals who cooperate for twenty years or more to raise their joint offspring, and because families are nested within subsistence groups, which, in turn, are nested within larger security and political groups, good decision-making skills will provide large benefits to mates as well as to members of one’s subsistence group or larger security and political groups. We further argue that decision-making that benefits others as well as oneself (joint utility optimization – JUO) can be especially computationally complex, and therefore that sexual selection and biological market forces favoring these skills would favor increased brain size. Finally, because parents must make decisions for their cognitively immature offspring, good JUO and other leadership abilities might have initially undergone strong selection in mothers, who provide most of the childcare in natural fertility populations.\nDecision-making that benefits others is one example of a valuable computational service. Other important examples include threat and opportunity detection, gossip and information sharing, cultural transmission, story telling, medicinal knowledge, and advice and counsel. Providing computational services in exchange for a variety of benefits would have helped subsidize a large, energetically expensive brain. Individuals who provided particularly valuable services gained prestige, i.e., additional benefits from fellow group members."
  },
  {
    "objectID": "posts/2017-12-05-academic-success-is-either-a-crapshoot-or-a-scam/index.html",
    "href": "posts/2017-12-05-academic-success-is-either-a-crapshoot-or-a-scam/index.html",
    "title": "Academic success is either a crapshoot or a scam",
    "section": "",
    "text": "Nature just published five brief commentaries by statisticians on the reproducibility crisis:\n\nAs debate rumbles on about how and how much poor statistics is to blame for poor reproducibility, Nature asked influential statisticians to recommend one change to improve science. The common theme? The problem is not our maths, but ourselves.\n\nThe problem is ourselves, but it has little to do with: a failure to adjust for human cognition (Leek), relying on statistical “significance” (McShane and Gelman), failure to appreciate false positive risk (Colquhoun), or not sharing analysis and results (Nuijten) (although addressing all these things would be good). Goodman, who fingers academic norms, comes closest, but doesn’t identify the real culprit.\nThe problem, in a nut shell, is that empirical researchers have placed the fates of their careers in the hands of nature instead of themselves.\nLet me explain.\nAcademic success for empirical researchers is largely determined by a count of one’s publications, and the prestige of the journals in which those publications appear (and the grants that flow from these). Prestigious journals, in turn, typically only publish papers that they deem to be reporting important new facts about the world.\nIn my field of anthropology, the minimum acceptable number of pubs per year for a researcher with aspirations for tenure and promotion is about three. This means that, each year, I must discover three important new things about the world.\nIs that realistic?\nResearch outcomes are stochastic – if we knew with 100% certainty what the outcome would be, why research it? The whole point of research is learn something new. When researchers begin a study they have some level of uncertainty – perhaps great, perhaps small – about the outcome. We all hope for a sexy outcome, but we all know that we might not get it.\nLet’s say I choose to run 3 studies that each has a 50% chance of getting a sexy result. If I run 3 great studies, mother nature will reward me with 3 sexy results only 12.5% of the time. I would have to run 9 studies to have about a 90% chance that at least 3 would be sexy enough to publish in a prestigious journal.\nI do not have the time or money to run 9 new studies every year.\nI could instead choose to investigate phenomena that are more likely to yield strong positive results. If I choose to investigate phenomena that are 75% likely to yield such results, for instance, I would only have to run about 5 studies (still too many) for mother nature to usually grace me with at least 3 positive results. But then I run the risk that these results will seem obvious, and not sexy enough to publish in prestigious journals.\nTo put things in deliberately provocative terms, empirical social scientists with lots of pubs in prestigious journals are either very lucky, or they are p-hacking.\nI don’t really blame the p-hackers. By tying academic success to high-profile publications, which, in turn, require sexy results, we academic researchers have put our fates in the hands of a fickle mother nature. Academic success is therefore either a crapshoot or, since few of us are willing to subject the success or failure of our careers to the roll of the dice, a scam.\nThe solution is straightforward: Although we have almost no control over the sexiness of our outcomes, we have almost full control over the quality of our studies. We can come up with clever designs that discriminate among popular hypotheses. We can invent new measures of important phenomena, and confirm their validity, reliability and precision. We can run high powered studies with representative samples of key populations. We can use experimental designs. We can pre-register our hypotheses and statistical tests. In short, we need to change the system so academic researchers are rewarded for running high quality studies with these sorts of attributes, regardless of outcome.\nChanging the incentives to reward high quality studies rather than sexy results would have enormously positive effects for science. Researchers will be able to respond to these incentives in ways that improve science while also advancing their careers. Under the current outcome-based incentives, in contrast, researchers often have little choice but to screw science to advance their careers.\nChanging the incentive system won’t be easy. No longer will we be able to easily assess our colleagues based on their number of pubs, weighted by journal impact factors. Instead, we will have to assess them based on the quality of their studies: the importance of the question addressed, the sampling strategy and sample size, the measurements and their ability to discriminate among hypotheses, and the data analysis. All these will have to be recorded in detail, even if there were no sexy results.\nChanging the incentive system might not only help solve the replication crisis, it might also help solve the serials crisis – the recent dramatic increase in the cost of subscribing to scientific journals.\nScientific publishing is an oligopoly. In the social sciences, 5 publishers – Elsevier, Taylor & Francis, Wiley-Blackwell, Springer, and Sage Publications – publish about 70% of all papers:\n\n\n\nPercent of journal articles in different disciplines that are published in the big 5. Figure from https://doi.org/10.1371/journal.pone.0127502\n\n\nThese publishers are exploiting their monopolies on journals and journal papers to charge high fees, which are mainly paid by university libraries. Reed-Elsevier’s profit margins, for example, exceed those of Apple, Google, or Amazon:\n\n\n\nReed-Elsevier profits. Left: entire business. Right: scientific, technical and medical division. Figure from: https://doi.org/10.1371/journal.pone.0127502.\n\n\nStudent tuition, grant, and endowment dollars are being funneled to highly profitable corporations that add only questionable value to the science they publish.\nNature, a big money maker for Springer (one of the scientific publishing oligopolists), tapped 5 statisticians for comment because it is worried about the replication crisis, and with good reason. Nature, as we all know, is the king of kingmakers in science because it only publishes the sexiest of sexy results. Social scientists, myself included, crave a publication in Nature, which can make one’s scientific career. But if those results are often hacked and cannot be replicated, Nature’s status will plummet, and along with the profits it generates for Springer. Yet Nature and the statisticians seem completely oblivious to the irony that it is the prestige of publishing sexy results in high profile journals like Nature that is the central cause of the replication crisis.\nUltimately, though, we academic researchers are responsible for both the replication and serials crises because we created (or bought into) a system that rewards sexy results over quality measurements of the world."
  },
  {
    "objectID": "posts/2018-01-16-our-statistics-dont-suck-our-theories-do/index.html",
    "href": "posts/2018-01-16-our-statistics-dont-suck-our-theories-do/index.html",
    "title": "Our statistics don’t suck, our theories do",
    "section": "",
    "text": "Every genuine test of a theory is an attempt to falsify it, or to refute it. Karl Popper, 1963.\nMost social scientists test a substantive hypothesis, which we will call \\(H_1\\), by instead testing a null hypothesis, \\(H_0\\), which is usually something like “the mean of X equals 0” or “the correlation of X and Y equals 0.” If the probability of the data1 under \\(H_0\\) is low (e.g., \\(p &lt; 0.05\\)), the researcher rejects \\(H_0\\), which is taken as evidence in favor of \\(H_1\\).\nThis approach, termed null hypothesis significance testing (NHST), has been subjected to scathing criticism. In the social sciences, for example, we know \\(H_0\\) is false before we collect a single data point — the mean of X is never exactly 0, the correlation of two variables is never exactly 0, and so forth. Hence, the rejection of such a null, which Cohen (1994) termed a “nil” hypothesis, just depends on sample size:\nFurther, the rejection of a nil \\(H_0\\) (which we already know to be false) is exceedingly weak evidence in favor of our pet hypothesis, \\(H_1\\). First, rejection of a nil is also probably consistent with many other hypotheses. Worse, if \\(H_1\\) predicts, e.g., the mean of X is greater than 0, the chance of being right is 50-50. Even with this incredibly weak standard, lots of social science studies don’t replicate.\nWe social scientists might be forgiven for concluding that NHST is deeply flawed. We would be wrong. The problem, instead, is our weak theories.\nIn the natural sciences, such as physics and chemistry, theories typically predict numerical values for various parameters. Sometimes the predicted value might be 0. Under Einstein’s theory of special relativity, for instance, the speed of light in the direction of the earth’s motion will not differ from the speed of light perpendicular to the earth’s motion. Other times, the predicted values are different from 0. Einstein’s theory of general relativity, for example, predicts that the sun will bend star light by a specific (non-zero) amount. In either case, the predicted value is a substantive \\(H_0\\) (not a nil) that can be meaningfully tested with NHST.\nUnlike rejecting a nil, which social scientists usually take as evidence supporting their theory, rejecting a substantive \\(H_0\\) rejects the theory! If the difference of the speed of light in one direction vs. another is greater than 0 \\((p &lt; 3 \\times 10^{-7})\\), then special relativity is wrong!2 If the sun bends light more or less than the predicted amount \\((p &lt; 3 \\times 10^{-7})\\) then general relativity is wrong!3 In the natural sciences, NHST is often a powerful tool to challenge theories, not support them (see Figure 1).\nFigure 1: Testing nils vs. substantive nulls. Black vertical line: null. Red vertical line: mean of data. A: Rejecting the nil that the mean = 0 is taken as support for a theory predicting that the mean has some value &gt; 0. B: Rejecting the substantive null that the mean = 0 rejects the theory. C: Failing to reject the substantive null that the mean = 3 fails to reject the theory. All data simulated.\nFalsifying a theory is an essential step toward developing a better theory.\nThis critical difference in the use of NHST in the natural vs. social sciences was pointed out by Paul Meehl in 1967:\nMost theories in the social sciences, including my own, are so weak that they can only predict that a value will be positive or negative, nothing more (e.g., Figure 1A). Such vague predictions make it harder to falsify these theories, therefore impeding development of better theories.\nYet it is certainly possible to develop social science theories that predict specific values, and thus expose themselves to meaningful challenges with NHST.\nMy graduate student Aaron Lightner and I, for example, recently missed an opportunity to do better science by following standard practice and testing a nil, when we now realize we should have also tested a substantive null. We conducted a classic framing effect study in which we predicted that participants would make different monetary offers in an ultimatum game if it was framed as a currency exchange than if it was “unframed.” We set up our statistical test in the standard social science way. \\(H_0\\) was a “nil”: no difference in mean offers in the framed vs. unframed condition.\nWe found a huge effect size (\\(d\\sim2\\); see Figure 2, and our p-value was very small \\((p = 1.8 \\times 10^{-31})\\). We therefore rejected the nil and concluded that there was a difference in mean offers. Publication!\nWe could have put our theory to a much more severe test, however, by proposing a substantive \\(H_0\\). In our study, participants self-reported what they thought was a fair offer, and they also made actual monetary offers. Our scientific (not statistical) hypothesis was that actual offers would match self-reported fair offers. These, in turn, would match a cultural norm for offers in currency exchange that differed from offers usually seen in the ultimatum game. If a participant reported that 3% was a fair offer, for instance, then he or she should have offered 3%.\nAs our substantive \\(H_0\\), we therefore should have proposed this: There will be no difference between actual offers and self-reported fair offers. If we had, we would have found that the probability of our data under such an \\(H_0\\) was very small4, leading us to reject \\(H_0\\), and therefore to reject our theory. Specifically, our data showed that although many actual offers were exactly, or very close to, self-reported fair offers (dots that fall on, or close to, the diagonal line), as our theory predicted, many other actual offers differed substantially from self-reported fair offers (dots that are far from the diagonal line), contrary to our prediction (Figure 3):\nWe don’t even need to compute a p-value to see that we can reject our substantive \\(H_0\\). Our scientific theory is wrong.\nRejecting a theory is cause for excitement, however, not despair. If a physicist did an experiment that convincingly rejected the null that the speed of light in a vacuum is the same in all inertial frames of reference, she would set off an explosion of research in her discipline and end up with a Nobel Prize.\nMake no mistake: Aaron and I were delighted that our study confirmed our prediction that offers in the framed conditions would deviate substantially from the unframed condition (clustering near 0 and 1, instead of near 0.50 as in the standard ultimatum game). But, contrary to our prediction, it also found that many participants said that it would be fair to offer X, but then made a very different offer. Why? Great question for a new study!\nIn the social sciences we need more “Holy Hand Grenade” theories — theories that do not simply make the very weak prediction that, e.g., \\(\\bar{X}&gt;0\\), which barely deserves to be called a prediction, but instead predict, e.g., that the number shall be three, no more, no less. Such theories can be subjected to severe tests (i.e., potentially falsified) using NHST. Rejecting a substantive \\(H_0 = 3\\), or failing to reject it, would then represent real scientific progress, not the flip of a coin."
  },
  {
    "objectID": "posts/2018-01-16-our-statistics-dont-suck-our-theories-do/index.html#footnotes",
    "href": "posts/2018-01-16-our-statistics-dont-suck-our-theories-do/index.html#footnotes",
    "title": "Our statistics don’t suck, our theories do",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMore precisely, for some test statistic \\(z\\), the p-value is the probability of finding a value of \\(z\\) equal to or more extreme than that observed, under the assumption that \\(H_0\\) is true.↩︎\nIn a recent example, the OPERA experiment mistakenly observed neutrinos traveling faster than light.↩︎\nPhysicists usually demand much smaller p-values than social scientists, e.g., the \\(5\\sigma\\) rule.↩︎\nTo compute confidence intervals, we would need to estimate the precision of our measurements, e.g., how closely do self-reported fair offers correspond to participants’ actual beliefs about fair offers? For a detailed example of parameter estimation for a simple physics problem, see Aguilar et al. 2015.↩︎"
  },
  {
    "objectID": "posts/2018-01-01-data-dredging-is-the-dionysian-soul-of-science/index.html",
    "href": "posts/2018-01-01-data-dredging-is-the-dionysian-soul-of-science/index.html",
    "title": "Data dredging is the Dionysian soul of science",
    "section": "",
    "text": "Good and Evil Angels. Plate 4 of The Marriage of Heaven and Hell. From The William Blake Archive: http://www.blakearchive.org\nYin and Yang. Apollo and Dionysus. Heaven and Hell. Id and Superego. Reason and Emotion. Spock and McCoy. Many intellectual and moral frameworks are structured around two, often opposing, elements. If one element gains the upper hand over the other, beware. William Blake, in The Marriage of Heaven and Hell, took aim at the imbalance he saw in Christian theology. God, Heaven, and the Good were the “passive that obeys reason,” our Apollonian side. Satan, Hell and Evil were the “active springing from Energy,” wrongly suppressed by the Church, the Dionysian excess that “leads to the palace of wisdom.”\nData analysis in the social sciences has two forms, one “good” that is highly developed and has many rules that supposedly will lead us to truth, and one “bad” that lives in the shadows, has few if any rules, and is frequently, but wrongly, vilified. This imbalance is crippling the social sciences."
  },
  {
    "objectID": "posts/2018-01-01-data-dredging-is-the-dionysian-soul-of-science/index.html#footnotes",
    "href": "posts/2018-01-01-data-dredging-is-the-dionysian-soul-of-science/index.html#footnotes",
    "title": "Data dredging is the Dionysian soul of science",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTukey was not the first to recognize the importance of exploring data, nor to clearly distinguish exploration from confirmation. De Groot made these points in 1956, for example, and even then he noted they were not new. Statistician Andrew Gelman recently raised the issue on his blog. Unlike others, however, Tukey devoted a chunk of his career to developing and promoting EDA. Much of his writing on the topic has an aphoristic flavor, which reminded me of Blake’s Proverbs of Hell. I recommend you read Tukey (1980); it’s short, with no math.↩︎"
  },
  {
    "objectID": "posts/2018-06-25-the-universal-genetic-program-and-the-custom-built-phenotype-implications-for-race-and-sex/index.html",
    "href": "posts/2018-06-25-the-universal-genetic-program-and-the-custom-built-phenotype-implications-for-race-and-sex/index.html",
    "title": "The universal genetic program and the custom-built phenotype: implications for race and sex",
    "section": "",
    "text": "The human genome, for many, represents differences, essential differences. The real spectre that such differences fuel racism, sexism, and eugenics has lead much of academia to downplay or even deny a role for the genome in human affairs.\nGeneticist David Reich confronted this spectre in his recent New York Times Op-Ed where he raised the alarm that scientists are on the verge of discovering genetic differences in intelligence, cognition, and behavior among the races:\nI agree. A study of a large European sample, just published in Nature Genetics, found numerous genetic variants associated with intelligence (Savage et al. 2018). Another just-published study found that social mobility was associated with education-linked genetic variants (Belsky et al. 2018). It is not hard to imagine that similar studies using global samples might find even more such variants, and that the distribution of these variants might differ across populations.1\nReich suggests we can avoid the racist implications of such discoveries by learning from the example of the biological differences between males and females:\nAccording to Reich, although we’re learning more about our genetic differences every day, when it comes to freedoms and opportunities we should just ignore all these new discoveries.\nI’m skeptical this approach will keep sexists and racists at bay.\nOne problem with Reich’s attempt at an analogy between sex and race is that his account of genetic differences between the sexes is deeply misleading. More importantly, Reich, though rightly drawing our attention to new and perhaps unsettling discoveries about the genome, is making the same fatal error many of us academics have made. By downplaying or ignoring the profound role of the genome in human affairs specifically, we have left a scientific vacuum that will be filled with racist and sexist claptrap.\nHere I want to sketch the scientific model of the genome that should fill that vacuum. Many of the key features of this model emerged from research on sea urchin development, which began in the 1840’s, more than a decade before Darwin published On the Origin of Species.\nFigure 1: Sea urchin development. Blue color indicates distribution of mRNA transcripts of activinB, a key signaling protein. Figure from Sethi et al. 2009.\nBefore tackling the genetics of sex and sea urchin development, though, let’s first get a handle on Reich’s scientific perspective on the genetic basis of race."
  },
  {
    "objectID": "posts/2018-06-25-the-universal-genetic-program-and-the-custom-built-phenotype-implications-for-race-and-sex/index.html#footnotes",
    "href": "posts/2018-06-25-the-universal-genetic-program-and-the-custom-built-phenotype-implications-for-race-and-sex/index.html#footnotes",
    "title": "The universal genetic program and the custom-built phenotype: implications for race and sex",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRegarding the construct “intelligence,” my views correspond pretty closely to Cosma Shalizi’s, see, e.g., here, here, here, and here. A taste: “If we must argue about the mind in terms of early-twentieth-century psychometric models, I’d suggest that Thomson’s is a lot closer than the factor-analytical ones to what’s suggested by the evidence from cognitive psychology, neuropsychology, functional brain imaging, general evolutionary considerations and, yes, evolutionary psychology (which I think well of, when it’s done right): that there are lots of mental modules, which are highly specialized in their information-processing, and that almost any meaningful task calls on many of them, their pattern of interaction shifting from task to task.”↩︎\nMore than 99.9% of the 4-5 million variants are single nucleotide polymorphisms (SNPs) and short insertions or deletions. However, it has recently been discovered that larger variable chunks of DNA, termed structural variants (SVs), though very few in number (the typical genome has a few thousand), affect more bases. See Sudmant et al. 2015 and The 1000 Genomes Project Consortium, 2015. According to Sudmant et al. 2015, when collapsing mCNV (multi allelic copy-number variants) sites “carrying multiple copies as well as homozygous SVs onto the haploid reference assembly, a median of 8.9Mbp of sequence are affected by SVs, compared to 3.6 Mbp for SNPs.” Most bases affected by SVs are either due to deletions or copy number variants. There is evidence of purifying selection against deletions in functional regions of the genome, but less so for copy number variants, indicating relaxed constraints on the latter. Common SVs are shared across continents whereas rare ones tend to be continent-specific.↩︎\nThe Y-chromosome has a very complex structure, including pseudoautosomal regions that recombine with the X-chromosome. There are some genes in the male-specific region of the Y-chromosome other than SRY that do play critical roles in the male phenotype, however. Hence, some of the male genetic program is encoded on the Y-chromosome, and not on the autosomes or X-chromosome. For review, see Jobling and Tyler-Smith 2017.↩︎"
  },
  {
    "objectID": "posts/2018-12-16-seven-reasons-why-most-major-depression-is-probably-not-a-brain-disorder/index.html",
    "href": "posts/2018-12-16-seven-reasons-why-most-major-depression-is-probably-not-a-brain-disorder/index.html",
    "title": "Seven reasons why most Major Depression is probably not a brain disorder",
    "section": "",
    "text": "Virtually all mental health researchers accept that Major Depression (MD) is a mental disorder, i.e., a brain dysfunction. I argue that this widespread belief should instead be treated as an untested hypothesis, and further, that this hypothesis is probably false. Instead, most MD in the general population is probably severe but normal sadness or grief. Here are seven reasons why:\n\nMost Major Depression is caused by adversity\nOrdinary sadness and grief are caused by adverse events. I suspect that a common view about MD is that it fundamentally different, striking without cause, out of the blue. Most studies of MD do not even bother to measure recent negative life events. There is a consensus, however, that MD, too, is caused in large part by adverse events, and that events of higher severity increase the risk of MD. Many early studies found that about 80% of cases of MD had evidence of at least one adverse event (compared to a much lower rate among non-cases). See Figure 1.\n\n\n\n\n\nFigure 1: Life events and onset of major depression. Figure and caption from Mazure (1998).\n\n\n\n\nMore recent studies using twin designs have found similar results, and additionally show the interaction between adversity and vulnerability factors such as female sex and neuroticism (see Figure 2).\n\n\n\n\n\nFigure 2: Figure from Kendler et al. (2004).\n\n\n\n\nThe stressful life events in Kendler et al. (2004) included:\n\nassault (assault, rape, or mugging)\ndivorce/separation (divorce, marital separation, broken engagement, or breakup of other romantic relationship),\nmajor financial problem\nserious housing problems\nserious illness or injury\njob loss (laid off from a job or fired)\nlegal problems (trouble with police or other legal trouble)\nloss of confidant (separation from other loved one or close friend other than a spouse or partner)\nserious marital problems (involving a marital or marriage-like intimate, cohabiting relation- ship)\nrobbed\nserious difficulties at work\nserious trouble getting along with a close social partner\nserious personal crisis of a close social partner\ndeath or illness of a close social partner\n\nMost of us hit by such adverse events would also experience low mood, sadness, loss of interest, insomnia, and other symptoms of MD. In fact, the study design of Kendler et al. (2004) instructed interviewers to rate the severity of the events as “what most people would be expected to feel about an event in a particular set of circumstances and biography….”\nThe causal effect of adversity on depression is most convincingly demonstrated by the finding that independent adverse events — those, like death of a loved one, that could not be caused by the depressed individual — are powerful predictors of MD.\n\n\nDiagnostic criteria for Major Depression were not developed to distinguish the ill from the healthy but instead to distinguish MD from other psychiatric disturbances\nAlmost all research on Major Depression (MD) diagnoses it using either the Diagnostic and Statistical Manual (DSM) criteria, or the very similar International Classification of Diseases (IDC) criteria. The current DSM-V criteria are basically the same as those in DSM-III, which initiated the modern era of depression research. These criteria are listed in the right-most column in Figure 3.\n\n\n\n\n\nFigure 3: Historical origins of the symptomatic criteria for Major Depression: Criteria proposed 1950-1980. Table and caption from Kendler et al. 2010.\n\n\n\n\nAt some point in their lives, everyone will experience at least one of these symptoms, and most of us will probably experience most of them. Prima facie, none indicate brain dysfunction. To be diagnosed with MD under the DSM criteria, a person must be experiencing 5 or more symptoms most of the day for at least two weeks, and at least one symptom must be sad or depressed mood (dysphoric mood) or loss of interest or pleasure.\nWhere did these criteria come from? You might think they came from studies designed to distinguish mental illness from normal sadness and grief. If so, you would be wrong. Instead, they come from studies that were conducted among groups of individuals who had already been determined to suffer from a variety of severe psychiatric disturbances (or physical illnesses). The goal of these studies was to develop criteria that would enable different psychiatrists to reliably provide the same mentally ill patient with the same diagnosis, such as bipolar disorder or schizophrenia, not to distinguish the mentally ill from the healthy.\nSpecifically, the DSM-III criteria can be traced to Stone and Burris (1950), which was a clinical study of 50 selected cases; Cassidy et al. (1957), which was a quantitative study of one hundred manic-depressive patients compared to fifty medically sick controls; Feighner et al. (1972), which was a study of 314 psychiatric emergency room patients and 87 psychiatric inpatients. And Spitzer et al. (1975), which tested the reliability of the Research Diagnostic Criteria (RDC) with 218 psychiatric inpatients. See Figure 3.\nNone of the studies that defined MD as we understand it today included any healthy participants, nor any identified as experiencing only ordinary sorrow, sadness, or grief. Hence, there is no reason to believe that, when applied to the general population, the criteria developed in these studies would effectively distinguish the tiny minority of individuals with a genuine mental illness from the much larger number of individuals who were suffering ordinary low mood, sadness, or grief.\nAs Kenneth Kendler, one of the world’s preeminent depression researchers admits, “most of the diagnostic categories and the diagnostic criteria they contain have been accepted for historical rather than strictly empirical reasons” (Lux and Kendler 2010).\n\n\nDSM criteria (mis)applied to community populations generated massive prevalence rates\nIt is no surprise, then, that when DSM criteria were first applied to the general population they generated implausibly high prevalence rates of mental illness. Over a quarter of the population (28.5%) was identified as suffering a mental illness in the last year, and nearly half the population (48%) as having suffered a mental illness in their lifetimes. For MD, up to 10% were identified to have suffered an episode in the last year, and 17% to have suffered MD in their lifetime. See Figure 4.\n\n\n\n\n\nFigure 4: Table from Regier et al. 1998\n\n\n\n\nRegier et al. 1998 acknowledged that these high prevalence rates called into question the validity of “diagnoses” based on DSM criteria in community populations:\n\nAlthough it is possible that all of these community-based disorders are simply milder cases of essentially the same disorders seen in clinical settings, there are other possibilities as well. Based on the high prevalence rates identified in both the ECA and the NCS, it is reasonable to hypothesize that some syndromes in the community represent transient homeostatic responses to internal or external stimuli that do not represent true psychopathologic disorders. The human organism has a limited repertoire of response patterns to various physical, biological, and emotional stresses. Transient changes in blood pressure, pulse rate, body temperature, anxiety, or mood are not always indicators of pathology but of appropriate adaptive responses. It is possible that many people with currently defined mental syndromes (in particular among the affective and anxiety disorders) not brought to clinical attention may be having appropriate homeostatic responses that are neither pathologic nor in need of treatment — eg, other equivalents of grief reactions that meet clinical criteria but are not considered pathologic if they are time-limited.\n\nSuch eminently reasonable interpretations of MD in community populations have virtually disappeared from the scientific literature, and high prevalence rates are now reported without a bat of the eye. In the US, for example, the government reports that about 1 in 5 adolescent women (19.4%) suffered MD in the past year, i.e., putatively suffered a major disorder of the brain.\nReally?\nRegier, first author of the above quote, went on to co-chair the DSM-5 Task Force, which, ironically, was widely criticized for further medicalizing normal reactions to common life experiences. The most prominent critic was Regier’s predecessor, Allen Frances, chair of the DSV-IV Task Force. See, for instance, his book:\n\n\n\n\n\nFigure 5: ?(caption)\n\n\n\n\nRobert Spitzer, chair of the DSM-III Task Force, expressed similar worries:\n\n\nRegier and colleagues shot back that Frances’ and Spitzer’s criticisms were motivated by the loss of royalties from sales of DSM-IV products. Frances scoffed that these royalties were a relative pittance.\n\n\nDepression is a continuum\nIf depression were a major brain disorder that could be identified by counting up symptoms that are common experiences, you might expect that folks with MD would stand apart from everyone else in the distribution of their symptoms. But they don’t. Although there is some debate, most studies have found that depression is dimensional, i.e., that it is “a quantitative elevation on a continuum of depression-relevant features found in all people” (Prisciandaro and Roberts 2005). As can be seen in Figure 6, there is no natural separation between those with lower depression scores and those with higher.\n\n\n\n\n\nFigure 6: Distribution of scores from the Patient Health Questionnaire (PHQ-9), with a conventional threshold of 10. Nationally representative US data from NHANES 2011-2012.\n\n\n\n\nWhen Cassidy, developer of one of the historical antecedents to the DSM-III MD criteria (see Figure 3), was asked how he decided on the threshold, he replied, “It sounded about right” (Kendler et al., 2010).\nThere is no principled reason to conclude that higher scores indicate brain disorder instead of more severe sadness.\n\n\nMajor Depression usually remits in a few months\nMany articles on MD emphasize that it is a chronic disease. This is not true for the majority of cases. The median duration of MD in a recent study of a nationally representative community sample was 6 months, and about 75% of cases remitted within a year. See Figure 7.\n\n\n\n\n\nFigure 7: Survival curves of a cohort (n = 393) with newly originated (first or recurrent) depressive episodes in the general population; +, censored cases. MinDD: minor depressive disorder. MDD: major depressive disorder. Figure and caption from ten Have et al. (2017).\n\n\n\n\nIn addition, the majority of individuals who suffer MD will have a single episode in their lifetimes. A recent study based on a large, nationally representative sample found that, among individuals in remission from MD at baseline, the cumulative recurrence rate was 13.4% at 10 years, and 27.1% at 20 years (Ten Have 2018).\n\n\nOnset of Major Depression is common at all adult ages\nIf MD were a genuine brain disorder, its epidemiology might resemble that of other genuine brain disorders, such as the epidemiology of developmental brain disorders, which occur early in life, or the epidemiology of brain disorders related to aging, i.e., those that occur late in life.\nTo compare MD with such brain disorders, I used the Institute for Health Metrics and Evaluation data visualization website to display results from the 2017 Global Burden of Disease Study. Here are the results for MD compared to three common developmental brain disorders:\n\n\n\n\n\nFigure 8: Major depression incidence compared to epidemiology of brain disorders that appear to be due to developmental disruption. Data from the Global Burden of Disease study 2017 and healthdata.org.\n\n\n\n\nAs you can see, new cases of MD are common starting in adolescence and throughout adulthood. In comparison, autism spectrum disorders are relatively rare and present at birth (hence prevalence rather than incidence is reported). Bipolar disorder and schizophrenia are also relatively rare, with peaks in incidence rates in late adolescence and early adulthood.\nSimilarly, although incidence of MD increases with age, it does not resemble other brain disorders related to aging, such as dementia, Parkinson’s or stroke, which are exceedingly rare until after the age of 40 or 50:\n\n\n\n\n\nFigure 9: Major depression incidence compared to epidemiology of brain disorders that appear to be due to aging. Data from the Global Burden of Disease study 2017 and healthdata.org.\n\n\n\n\n\n\nNeurophysiological differences associated with MD do not necessarily indicate brain deficits\nMD’s status as a “real” illness is often justified by pointing to biological differences associated with depression. The Mayo Clinic, for example, on its info page for MD, only lists biological factors as causes of MD, such as physical changes in the brain, brain chemistry, hormones, and genetics (adversity, in contrast, is a “risk factor”). Several biological differences associated with MD are depicted in Figure 10:\n\n\n\n\n\nFigure 10: Biological systems involved in the pathophysiology of MDD. Clinical studies in major depressive disorder (MDD) and relevant animal models have identified pathophysiological features in the central nervous system, as well as the major stress response systems, such as the hypothalamic–pituitary–adrenal (HPA) axis, the autonomic nervous system and the immune system. In the central nervous system, altered neurotransmission and reduced plasticity are evident. These could underlie functional changes in relevant brain circuits (for example, cognitive control and affective–salience networks), smaller regional brain volumes (for example, in the hippocampus) and neuroinflammation, as confirmed in neuroimaging studies. Beyond the central nervous system, chronic hyperactivity impairs feedback regulation of the HPA axis, which is one of the most consistently reported biological features of MDD. Within the immune system, substantial evidence supports increased levels of circulating cytokines and low-grade chronic activation of innate immune cells, including monocytes. However, other aspects of immunity seem to be impaired as exemplified by reduced natural killer (NK) cell cytotoxicity and T cell proliferative capacity. Once it becomes chronic, both HPA axis hyperactivity and inflammation might converge with alterations in the autonomic nervous system to contribute to central nervous system pathobiology as well as cardiovascular and metabolic disease, which often co-occur with MDD. The sequence of events leading to changes in these interconnected systems and their exact relationship is not known. However, mechanistic studies in animals have shown that alterations in stress response systems can directly and indirectly affect the central nervous system (BOX 3). Conversely, chronic stress and associated changes in behaviour can reproduce many of the stress system alterations, including HPA feedback impairment and inflammation, which suggests a bidirectional link between central and peripheral biological features of MDD. ACTH, adrenocorticotropin; CRH, corticotropin-releasing hormone; TNF, tumour necrosis factor. Figure and caption from Otte et al. (2016).\n\n\n\n\nThe brain, however, is a biochemical machine. It is not surprising that individuals in different emotional states, such as depressed vs. non-depressed, have differences in brain neurophysiology. All brain functions involve neurophysiological changes in the brain. Your brain is undergoing countless chemical changes as you read this post. If you remember anything you’ve read, for example, your brain has undergone some long-term chemical changes in synaptic connections between neurons. Differences in subjective experiences must be caused by physical differences in the brain. Indeed, if there weren’t biochemical and neurophysiological changes underlying MD this would be a shocking finding that would shake our materialist conception of the brain to its core.\nMoreover, none of the studies I’ve seen of neurophysiological and biochemical differences in those with MD could distinguish differences from deficits, even in principle. Every study I’ve looked at has the following design: a group of participants that meet diagnostic criteria for MD are compared to a group of “healthy” controls, i.e., individuals without MD. But most of the individuals with MD are (1) experiencing sadness or low mood (one of two necessary symptoms), and (2) have suffered recent adversity. Most members of the control group, in contrast, are not experiencing sadness or low mood, and have not suffered recent adversity. Hence, MD is almost completely confounded with sadness and recent adversity.\nAll of the neurophysiological and other differences in Figure 10 that are identified as “impairments” or “pathophysiological features” are simply differences whose implications are currently unknown. They could easily be some of the biological bases of normal sadness and other functional responses to adversity.\n\n\nSummary\nMD is caused by adversity, such as loss of a loved one; it is characterized by symptoms such as sadness, low mood, and loss of interest, which most people experience when they experience adversity; it is diagnosed when the count of such common symptoms exceeds an arbitrary threshold; it is common among adolescents and adults of every age in the general population; it usually lasts for no more than a year; and most people will experience at most one episode in their lifetimes. Taken together, these facts provide considerable evidence for the hypothesis that most MD in the general population is simply severe sadness or grief.\nI am far from the only one to make this argument. Horwitz and Wakefield wrote a book on it (note the Forward by Robert Spitzer, chair of the DSM-III effort):\n\n\n\n\n\nFigure 11: The Loss of Sadness. Horwitz and Wakefield.\n\n\n\n\nIf these arguments are correct, why do mental health researchers cling to the axiom that MD is a serious brain disorder, instead of treating this as a hypothesis? Here are a few possible reasons:\n\nMany psychiatrists form their initial intuitions about MD by working with inpatients. MD in these clinical populations can look quite different from MD in the general population. Because these individuals refer themselves, or are referred by family members, for psychiatric treatment, their MD tends to be chronic, recurring, and have little connection to recent adverse events, making it seem much less like “normal” sadness or grief. Such rarer forms of MD are better candidates for a genuine disorder.\n$$$. The NIH budget for depression research is approaching half a billion dollars annually ($466 billion in 2018). Sales of antidepressants are generating revenues of around $11.6 billion annually. And Big Pharma kicks back a lot of bucks to “thought leaders” in medicine, including psychiatry. The flow of all these dollars depends on MD being a serious illness rather than ordinary sadness.\nSzasz was right: psychiatry aims to control undesirable behavior, and severe sadness and grief are aversive to sufferers and often inconvenient for social partners and employers.\nIt’s politically incorrect to question the illness axiom, perhaps because it runs counter to the laudable effort to reduce the stigma of mental illness.\n\nRegarding #4, viewing mental illness as an ‘illness like any other’ and promoting biogenetic causes have been thought to reduce stigma. Larkings and Brown (2018) conducted a systematic review of the impact of biogenetic beliefs regarding mental illness on stigma. They found, on the contrary, that among the public:\n\nOverall, these reviews suggest that there are mixed ramifications associated with public endorsement of biogenetic causes. While biogenetic causes might help reduce blame, they might also contribute to negative consequences, such as increased perceived dangerousness, social distance, and pessimism around prognosis.\n\nAmong the mentally ill and mental health professionals, things were even worse:\n\nThe present review indicates that the endorsement of biogenetic causes does not reduce stigma in people with mental illness and in mental health professionals, and might actually increase stigma and negative attitudes towards mental illness.\n\nFinally, even if most MD is normal sadness and grief sufferers will often still need help, sometimes from professionals. Much of my research and that of my grad students explores the possibility that some aspects of MD function to signal need to social partners. Treatment, though, would focus on solving real-life problems rather than altering brain chemistry.\nYou can find papers on my evolutionary approach to depression here:\nhttps://anthro.vancouver.wsu.edu/people/hagen/depression-and-other-mental-health-issues-in-evolutionary-perspective/\nAddendum (2018/12/21)\nIf most MD, as it is currently diagnosed, is not disorder, should we keep calling it Major Depression? Wakefield’s answer, and I think it’s a good one, is no. The term Major Depression should be reserved for genuine disorder, leaving many, if not most, cases of MD in the community as false positives (e.g., Wakefield and Schmitz 2013; Wakefield 2016). That is, the criteria in Figure 3 are insufficient to distinguish functional responses to adversity, such as sadness and grief, from disordered responses. Additional criteria that might reduce false positives include MD-levels of symptoms in the absence of adversity, that the severity of the response is disproportionate to the severity of the adverse event, persistence of symptoms well past the adverse event (e.g., more than a year), or a high rate of recurrence.\n\n2019/09/22: rearranged order of sections; minor edits for clarity\n2022/05/22: updated epidemiology plots"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Recent posts",
    "section": "",
    "text": "Is evolutionary psychology impossible?\n\n\n\n\n\n\nscience\n\n\n\n\n\n\n\n\n\nJan 21, 2020\n\n\nEd Hagen\n\n\n\n\n\n\n\n\n\n\n\n\nAbout 90% of the genome is junk, which is very informative about ancestry but says little about biology\n\n\n\n\n\n\nscience\n\n\n\n\n\n\n\n\n\nJul 27, 2019\n\n\nEd Hagen\n\n\n\n\n\n\n\n\n\n\n\n\nShould scientific publishing move to Github and friends?\n\n\n\n\n\n\nscience\n\n\n\n\n\n\n\n\n\nJul 12, 2019\n\n\nEd Hagen\n\n\n\n\n\n\n\n\n\n\n\n\nA theory of natural selection, 5th century BC\n\n\n\n\n\n\nscience\n\n\n\n\n\n\n\n\n\nJun 20, 2019\n\n\nEd Hagen\n\n\n\n\n\n\n\n\n\n\n\n\nMeasles, mothers, leadership, and the evolution of big brains\n\n\n\n\n\n\nscience\n\n\n\n\n\n\n\n\n\nMar 21, 2019\n\n\nEd Hagen\n\n\n\n\n\n\n\n\n\n\n\n\nSeven reasons why most Major Depression is probably not a brain disorder\n\n\n\n\n\n\nscience\n\n\n\n\n\n\n\n\n\nDec 16, 2018\n\n\nEd Hagen\n\n\n\n\n\n\n\n\n\n\n\n\nSuicide and #MeToo\n\n\n\n\n\n\nscience\n\n\n\n\n\n\n\n\n\nOct 9, 2018\n\n\nEd Hagen\n\n\n\n\n\n\n\n\n\n\n\n\nMost shooters are suicidal. Would arming teachers deter them?\n\n\n\n\n\n\nscience\n\n\n\n\n\n\n\n\n\nAug 23, 2018\n\n\nEd Hagen\n\n\n\n\n\n\n\n\n\n\n\n\nThe universal genetic program and the custom-built phenotype: implications for race and sex\n\n\n\n\n\n\nscience\n\n\n\n\n\n\n\n\n\nJun 25, 2018\n\n\nEd Hagen\n\n\n\n\n\n\n\n\n\n\n\n\nWhile it may be true that Evolutionary Anthropologists consider themselves scientists…\n\n\n\n\n\n\nscience\n\n\n\n\n\n\n\n\n\nMar 3, 2018\n\n\nEd Hagen\n\n\n\n\n\n\n\n\n\n\n\n\nOur statistics don’t suck, our theories do\n\n\n\n\n\n\nscience\n\n\nstats\n\n\n\n\n\n\n\n\n\nJan 16, 2018\n\n\nEd Hagen\n\n\n\n\n\n\n\n\n\n\n\n\nData dredging is the Dionysian soul of science\n\n\n\n\n\n\nscience\n\n\nstats\n\n\n\n\n\n\n\n\n\nJan 1, 2018\n\n\nEd Hagen\n\n\n\n\n\n\n\n\n\n\n\n\nAcademic success is either a crapshoot or a scam\n\n\n\n\n\n\nscience\n\n\nstats\n\n\n\n\n\n\n\n\n\nDec 5, 2017\n\n\nEd Hagen\n\n\n\n\n\n\n\n\n\n\n\n\nPCA: new coordinate system, same data\n\n\n\n\n\n\nstats\n\n\n\n\n\n\n\n\n\nNov 15, 2017\n\n\nEd Hagen\n\n\n\n\n\n\n\n\n\n\n\n\nPut your data in an R package\n\n\n\n\n\n\nstats\n\n\n\n\n\n\n\n\n\nOct 18, 2017\n\n\nEd Hagen\n\n\n\n\n\n\n\n\n\n\n\n\nMonkey butts, menstrual cycles, sex, and the color pink. The statistical crisis in science\n\n\n\n\n\n\nscience\n\n\nstats\n\n\n\n\n\n\n\n\n\nJun 27, 2015\n\n\nEd Hagen\n\n\n\n\n\n\n\n\n\n\n\n\nPavlov’s dogs, dopamine and drug use\n\n\n\n\n\n\nscience\n\n\n\n\n\n\n\n\n\nMay 16, 2015\n\n\nEd Hagen\n\n\n\n\n\n\n\n\n\n\n\n\nI’m a sucker for a good theory\n\n\n\n\n\n\nscience\n\n\n\n\n\n\n\n\n\nMay 15, 2015\n\n\nEd Hagen\n\n\n\n\n\n\n\n\n\n\n\n\nIs pregnancy immunosuppression a myth?\n\n\n\n\n\n\nscience\n\n\n\n\n\n\n\n\n\nApr 19, 2015\n\n\nEd Hagen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Ed Hagen",
    "section": "",
    "text": "I am a Professor of Anthropology at Washington State University"
  }
]